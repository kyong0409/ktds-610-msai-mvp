"""
ì§€ì‹ ê´€ë¦¬ ì„œë¹„ìŠ¤ ë¡œì§
"""
import streamlit as st
import time
from datetime import datetime
from typing import Dict, Any, Tuple, Optional
from utils.file_processor import FileProcessor
from services.document_analyzer import DocumentAnalyzer
from markitdown import MarkItDown
from langchain_openai import AzureChatOpenAI
from dotenv import load_dotenv
import os
from azure.core.credentials import AzureKeyCredential
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
AZURE_DOCUMENT_INTELLIGENCE_KEY = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_KEY")

key = AzureKeyCredential(AZURE_DOCUMENT_INTELLIGENCE_KEY)

# Azure OpenAI ì„¤ì •
endpoint = os.environ["AZURE_ENDPOINT"]
api_key = os.environ["AZURE_AI_FOUNDRY_KEY"]
api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-12-01-preview")

print("endpoint: ", endpoint)
print("api_key: ", api_key)
print("api_version: ", api_version)

# ë°°í¬ ì´ë¦„: Foundryì—ì„œ ë§Œë“  ë°°í¬ëª…ê³¼ ë™ì¼í•´ì•¼ í•¨
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4.1-mini")
print("DEPLOYMENT_NAME: ", DEPLOYMENT_NAME)
llm = AzureChatOpenAI(
		azure_endpoint=endpoint,
		api_key=api_key,
		api_version=api_version,
		azure_deployment=DEPLOYMENT_NAME,
		temperature=0.4,
		max_retries=3,
		timeout=30,
)

class KnowledgeService:
    """ì§€ì‹ ê´€ë¦¬ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        self.file_processor = FileProcessor()
        self.document_analyzer = DocumentAnalyzer()
        self.markitdown = MarkItDown(docintel_endpoint=AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT, docintel_credential=key)

    def process_uploaded_file(self, uploaded_file) -> Tuple[Optional[Dict], Optional[str]]:
        """ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬"""
        # íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
        is_valid, error_message = self.file_processor.validate_file(uploaded_file)

        if not is_valid:
            st.error(error_message)
            return None, None

        # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        file_info = self.file_processor.get_file_info(uploaded_file)

        # íŒŒì¼ ë‚´ìš© ì¶”ì¶œ
        file_content = self.file_processor.extract_text(uploaded_file)

        return file_info, file_content

    def convert_file_to_text(self, uploaded_file) -> Tuple[bool, str]:
        """MarkItDownì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            import tempfile
            import os

            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_file_path = tmp_file.name

            # MarkItDownìœ¼ë¡œ ë³€í™˜
            result = self.markitdown.convert(tmp_file_path)

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)

            # ì—…ë¡œë“œ íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            uploaded_file.seek(0)

            return True, result.text_content

        except Exception as e:
            # ì—…ë¡œë“œ íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            uploaded_file.seek(0)

            # Fallback: ê¸°ì¡´ íŒŒì¼ ì²˜ë¦¬ ë°©ì‹ ì‚¬ìš©
            fallback_content = self._fallback_text_extraction(uploaded_file)
            if fallback_content:
                return True, fallback_content

            return False, f"íŒŒì¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def _fallback_text_extraction(self, uploaded_file) -> str:
        """Fallback í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹)"""
        try:
            # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            uploaded_file.seek(0)

            file_type = uploaded_file.type

            if file_type == "text/plain":
                return str(uploaded_file.read(), "utf-8")
            elif file_type == "application/pdf":
                try:
                    import PyPDF2
                    import io

                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.read()))
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                    return text
                except:
                    return f"[PDF íŒŒì¼: {uploaded_file.name}] - PDF ë³€í™˜ì„ ìœ„í•´ 'pip install markitdown[pdf]'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            elif file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                try:
                    from docx import Document
                    import io

                    doc = Document(io.BytesIO(uploaded_file.read()))
                    text = ""
                    for paragraph in doc.paragraphs:
                        text += paragraph.text + "\n"
                    return text
                except:
                    return f"[Word íŒŒì¼: {uploaded_file.name}] - íŒŒì¼ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                return f"[{uploaded_file.name}] - ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."

        except Exception as e:
            return f"[íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜] {str(e)}"
        finally:
            # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            uploaded_file.seek(0)

    def get_file_preview(self, uploaded_file) -> str:
        """íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸ ë°˜í™˜"""
        success, content = self.convert_file_to_text(uploaded_file)

        if success:
            # ë¯¸ë¦¬ë³´ê¸°ëŠ” ì²˜ìŒ 1000ìë§Œ í‘œì‹œ
            preview_text = content[:1000]
            if len(content) > 1000:
                preview_text += "\n\n... (ê³„ì†)"
            return preview_text
        else:
            return content  # ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜

    def analyze_document(self, content: str, filename: str, settings: Dict = None) -> Dict:
        """ë¬¸ì„œ ë¶„ì„ ì‹¤í–‰"""
        
        template = """
        [ì—­í• ]
        ë‹¹ì‹ ì€ IT íšŒì‚¬ì˜ ì§€ì‹ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì…ë ¥ëœ í…ìŠ¤íŠ¸ ë¬¸ì„œ(ì›ë³¸ ì§€ì‹ ë¬¸ì„œ)ë¥¼ ë¶„ì„í•˜ì—¬, ì§€ì‹ìì‚°í™”ì— í•„ìš”í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ê°œì„ ì´ í•„ìš”í•œ ë³´ì™„ì ì„ ë„ì¶œí•˜ì„¸ìš”.

        [ì…ë ¥]
        {content}

        [ì§€ì‹œì‚¬í•­]
        ë‹¤ìŒ í•­ëª©ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        ## 1. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        - ë¬¸ì„œ ì¢…ë¥˜: {{PoC ë³´ê³ ì„œ | Lessons Learned | ê¸°ìˆ ìë£Œ | í”„ë¡œì íŠ¸ ì‚°ì¶œë¬¼ | ê¸°íƒ€}}
        - ì£¼ì œ(Topic): í•œ ì¤„ ìš”ì•½
        - ì‘ì„±ì¼/ì‘ì„±ì: ì›ë¬¸ì—ì„œ ë°œê²¬ë˜ë©´ ì¶”ì¶œ, ì—†ìœ¼ë©´ "ë¯¸í™•ì¸"
        - í”„ë¡œì íŠ¸/ì ìš© ë¶„ì•¼: ë¬¸ë§¥ì—ì„œ ìœ ì¶”
        - ì£¼ìš” í‚¤ì›Œë“œ(íƒœê·¸): í•µì‹¬ ê¸°ìˆ , ë„ë©”ì¸, ê´€ë ¨ ìš©ì–´ë¥¼ 5~10ê°œ

        ## 2. ë¬¸ì„œ êµ¬ì¡°/ëª©ì°¨ ë¶„ì„
        - ë¬¸ì„œ ë‚´ ì¡´ì¬í•˜ëŠ” ì£¼ìš” ì„¹ì…˜/í•­ëª© ëª©ë¡í™”
        - ê° ì„¹ì…˜ì´ ë‹¤ë£¨ëŠ” ë‚´ìš© ìš”ì•½

        ## 3. í™œìš© ê°€ëŠ¥ì„± ë¶„ì„
        - ì´ ë¬¸ì„œê°€ ì§€ì‹ìì‚°ìœ¼ë¡œì„œ ì–´ë–¤ ê°€ì¹˜ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆëŠ”ì§€
        - ì¬ì‚¬ìš©/ì°¸ì¡° ê°€ëŠ¥í•œ ë¶€ë¶„

        ## 4. ë³´ì™„ì´ í•„ìš”í•œ ì 
        - ë¹ ì§„ í•­ëª© (ì˜ˆ: ëª©ì , ê²°ê³¼, êµí›ˆ, ì ìš© ë°©ì•ˆ ë“±)
        - ë¶ˆëª…í™•í•˜ê±°ë‚˜ ì •ë¦¬ë˜ì§€ ì•Šì€ ë¶€ë¶„
        - ê²€ìƒ‰/ì¬ì‚¬ìš© ê´€ì ì—ì„œ ê°œì„ í•´ì•¼ í•  ì 

        [ì¶œë ¥ í˜•ì‹]
        ì•„ë˜ JSON êµ¬ì¡°ë¡œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

        ```json
        {{
            "metadata": {{
                "type": "",
                "topic": "",
                "author": "",
                "date": "",
                "project_area": "",
                "keywords": []
            }},
            "structure": [
                {{
                "section": "",
                "summary": ""
                }}
            ],
            "usability": "ì´ ë¬¸ì„œê°€ ì§€ì‹ìì‚°ìœ¼ë¡œì„œ ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆëŠ”ì§€ ì„¤ëª…",
            "improvements": [
                "ë³´ì™„ì 1",
                "ë³´ì™„ì 2",
                "ë³´ì™„ì 3"
            ]
        }}
        """

        # LLM ë¶„ì„ ì‹œë„
        try:
            prompt = PromptTemplate.from_template(template)
            chain = prompt | llm | StrOutputParser()

            print("content: ", content[:50])

            response = chain.invoke({
                "content": content
            })

            # JSON ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ê¸°ì¡´ êµ¬ì¡°ì™€ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜
            import json
            # JSON ë¸”ë¡ì—ì„œ ì‹¤ì œ JSONë§Œ ì¶”ì¶œ
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response

            llm_result = json.loads(json_str)

            # ê¸°ì¡´ DocumentAnalyzer êµ¬ì¡°ì™€ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜
            enhanced_content = self._create_enhanced_content(content, llm_result)
            return {
                "original_content": content,
                "enhanced_content": enhanced_content,
                "quality_score": self._calculate_quality_score(content, llm_result),
                "original_length": len(content),
                "enhanced_length": len(enhanced_content),
                "issues_found": llm_result.get("improvements", []),
                "improvements": llm_result.get("improvements", []),
                "metadata": {
                    "analyzed_at": datetime.now(),
                    "filename": filename,
                    "analyzer_version": "LLM-1.0",
                    "llm_metadata": llm_result.get("metadata", {})
                },
                "llm_analysis": llm_result
            }
        except Exception as e:
            # LLM ë¶„ì„ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ DocumentAnalyzerë¡œ fallback
            print(f"LLM ë¶„ì„ ì‹¤íŒ¨ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë˜ëŠ” íŒŒì‹± ì‹¤íŒ¨), fallback ì‚¬ìš©: {e}")
            return self._fallback_analysis(content, filename, str(e))

    def _create_enhanced_content(self, original_content: str, llm_result: Dict) -> str:
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–¥ìƒëœ ì½˜í…ì¸  ìƒì„±"""
        metadata = llm_result.get("metadata", {})
        structure = llm_result.get("structure", [])
        usability = llm_result.get("usability", "")
        improvements = llm_result.get("improvements", [])

        enhanced = f"""# {metadata.get('topic', 'ë¬¸ì„œ ë¶„ì„ ê²°ê³¼')}

## ğŸ“‹ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
- **ë¬¸ì„œ ì¢…ë¥˜**: {metadata.get('type', 'ë¯¸ë¶„ë¥˜')}
- **ì£¼ì œ**: {metadata.get('topic', 'ë¯¸í™•ì¸')}
- **ì‘ì„±ì**: {metadata.get('author', 'ë¯¸í™•ì¸')}
- **ì‘ì„±ì¼**: {metadata.get('date', 'ë¯¸í™•ì¸')}
- **í”„ë¡œì íŠ¸/ë¶„ì•¼**: {metadata.get('project_area', 'ë¯¸í™•ì¸')}
- **í‚¤ì›Œë“œ**: {', '.join(metadata.get('keywords', []))}

## ğŸ“‘ ì›ë³¸ ë‚´ìš©
{original_content}

## ğŸ” ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
"""
        for section in structure:
            enhanced += f"\n### {section.get('section', 'ì„¹ì…˜')}\n{section.get('summary', 'ë‚´ìš© ìš”ì•½')}\n"

        enhanced += f"""
## ğŸ’¡ í™œìš© ê°€ëŠ¥ì„±
{usability}

## âœ¨ ê°œì„  ì œì•ˆì‚¬í•­
"""
        for i, improvement in enumerate(improvements, 1):
            enhanced += f"{i}. {improvement}\n"

        enhanced += """
---
*ë³¸ ë¬¸ì„œëŠ” AI ì§€ì‹ê´€ë¦¬ ì‹œìŠ¤í…œì— ì˜í•´ ë¶„ì„ ë° ë³´ì™„ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
        return enhanced.strip()

    def _calculate_quality_score(self, content: str, llm_result: Dict) -> int:
        """ì½˜í…ì¸ ì™€ LLM ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        base_score = 60

        # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
        length_score = min(len(content) // 100, 20)

        # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
        metadata = llm_result.get("metadata", {})
        metadata_score = 0
        for key in ["type", "topic", "author", "date", "project_area"]:
            if metadata.get(key) and metadata[key] != "ë¯¸í™•ì¸" and metadata[key] != "":
                metadata_score += 2

        # í‚¤ì›Œë“œ ê°œìˆ˜
        keywords_score = min(len(metadata.get("keywords", [])) * 1, 10)

        # êµ¬ì¡° ë¶„ì„ í’ˆì§ˆ
        structure_score = min(len(llm_result.get("structure", [])) * 3, 15)

        total_score = base_score + length_score + metadata_score + keywords_score + structure_score
        return min(total_score, 100)

    def _fallback_analysis(self, content: str, filename: str, error_msg: str) -> Dict:
        """LLM ë¶„ì„ ì‹¤íŒ¨ì‹œ fallback ë¶„ì„"""
        basic_result = self.document_analyzer.analyze_document(content, filename)

        # ì—ëŸ¬ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
        basic_result["metadata"]["error_info"] = {
            "llm_error": error_msg,
            "fallback_used": True,
            "error_time": datetime.now()
        }
        basic_result["metadata"]["analyzer_version"] = "Fallback-1.0"

        # ì—ëŸ¬ ê´€ë ¨ ê°œì„ ì‚¬í•­ ì¶”ê°€
        if "Connection error" in error_msg or "getaddrinfo failed" in error_msg:
            basic_result["issues_found"].insert(0, "âš ï¸ AI ë¶„ì„ ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨ - ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”")
            basic_result["improvements"].insert(0, "ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸ í›„ ì¬ì‹œë„")

        return basic_result

    def _expert_analysis(self, content: str, filename: str) -> Dict:
        """ì „ë¬¸ê°€ ìˆ˜ì¤€ ë¶„ì„"""
        basic_result = self.document_analyzer.analyze_document(content, filename)

        # ì „ë¬¸ê°€ ë¶„ì„ ì¶”ê°€ ìš”ì†Œ
        expert_additions = {
            "expert_insights": [
                "ì—…ê³„ í‘œì¤€ê³¼ì˜ ë¹„êµ ë¶„ì„ í•„ìš”",
                "ìµœì‹  ë™í–¥ ë°˜ì˜ ê¶Œì¥",
                "ì‹¤ë¬´ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì¶”ê°€ ì œì•ˆ",
                "ì •ëŸ‰ì  ë°ì´í„° ë³´ê°• í•„ìš”"
            ],
            "technical_assessment": {
                "complexity_level": "ì¤‘ê¸‰",
                "target_audience": "ì‹¤ë¬´ì§„",
                "implementation_difficulty": "ë³´í†µ"
            },
            "quality_score": min(basic_result["quality_score"] + 10, 100)  # ì „ë¬¸ê°€ ë¶„ì„ìœ¼ë¡œ í’ˆì§ˆ ì ìˆ˜ í–¥ìƒ
        }

        # ê¸°ë³¸ ê²°ê³¼ì— ì „ë¬¸ê°€ ë¶„ì„ ì¶”ê°€
        result = {**basic_result, **expert_additions}
        return result

    def _detailed_analysis(self, content: str, filename: str) -> Dict:
        """ìƒì„¸ ë¶„ì„"""
        basic_result = self.document_analyzer.analyze_document(content, filename)

        # ìƒì„¸ ë¶„ì„ ì¶”ê°€ ìš”ì†Œ
        detailed_additions = {
            "detailed_metrics": {
                "readability_score": 78,
                "technical_accuracy": 85,
                "completeness_ratio": 0.75
            },
            "section_analysis": [
                {"section": "ê°œìš”", "completeness": 90, "quality": 85},
                {"section": "ë³¸ë¬¸", "completeness": 70, "quality": 80},
                {"section": "ê²°ë¡ ", "completeness": 60, "quality": 75}
            ],
            "quality_score": min(basic_result["quality_score"] + 5, 100)  # ìƒì„¸ ë¶„ì„ìœ¼ë¡œ í’ˆì§ˆ ì ìˆ˜ í–¥ìƒ
        }

        result = {**basic_result, **detailed_additions}
        return result

    def save_to_vector_db(self, analysis_result: Dict) -> Dict:
        """VectorDBì— ì €ì¥"""
        try:
            vector_entry = {
                "content": analysis_result['enhanced_content'],
                "filename": st.session_state.current_file_info['name'],
                "metadata": analysis_result.get('metadata', {}),
                "quality_score": analysis_result['quality_score'],
                "timestamp": datetime.now(),
                "original_length": analysis_result['original_length'],
                "analysis_type": st.session_state.get('analysis_settings', {}).get('depth', 'ê¸°ë³¸')
            }

            if 'vector_db' not in st.session_state:
                st.session_state.vector_db = []

            st.session_state.vector_db.append(vector_entry)

            return {
                "success": True,
                "message": "VectorDB ì €ì¥ ì™„ë£Œ",
                "count": len(st.session_state.vector_db)
            }

        except Exception as e:
            return {
                "success": False,
                "message": f"ì €ì¥ ì‹¤íŒ¨: {str(e)}"
            }

    def save_to_board(self, analysis_result: Dict) -> Dict:
        """ê²Œì‹œíŒì— ì €ì¥"""
        try:
            board_post = {
                "title": f"[AI ë³´ì™„] {st.session_state.current_file_info['name']}",
                "content": analysis_result['enhanced_content'],
                "author": "AI Knowledge System",
                "timestamp": datetime.now(),
                "views": 0,
                "quality_score": analysis_result['quality_score'],
                "file_info": st.session_state.current_file_info,
                "issues_found": analysis_result['issues_found'],
                "improvements": analysis_result['improvements'],
                "analysis_type": st.session_state.get('analysis_settings', {}).get('depth', 'ê¸°ë³¸')
            }

            if 'board_posts' not in st.session_state:
                st.session_state.board_posts = []

            st.session_state.board_posts.append(board_post)

            return {
                "success": True,
                "message": "ê²Œì‹œíŒ ë“±ë¡ ì™„ë£Œ",
                "count": len(st.session_state.board_posts)
            }

        except Exception as e:
            return {
                "success": False,
                "message": f"ë“±ë¡ ì‹¤íŒ¨: {str(e)}"
            }

    def bulk_vector_to_board(self) -> Dict:
        """VectorDB ë¬¸ì„œë“¤ì„ ê²Œì‹œíŒìœ¼ë¡œ ì¼ê´„ ì´ë™"""
        vector_db = st.session_state.get('vector_db', [])

        if not vector_db:
            return {
                "success": False,
                "message": "VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤",
                "count": 0
            }

        if 'board_posts' not in st.session_state:
            st.session_state.board_posts = []

        moved_count = 0
        for doc in vector_db:
            # ì´ë¯¸ ê²Œì‹œíŒì— ìˆëŠ”ì§€ í™•ì¸
            existing = any(
                post.get('title', '').endswith(doc.get('filename', ''))
                for post in st.session_state.board_posts
            )

            if not existing:
                board_post = {
                    "title": f"[VectorDB ì´ë™] {doc.get('filename', 'Unknown')}",
                    "content": doc.get('content', ''),
                    "author": "Knowledge Management System",
                    "timestamp": datetime.now(),
                    "views": 0,
                    "quality_score": doc.get('quality_score', 0),
                    "file_info": {"name": doc.get('filename', 'Unknown')},
                    "issues_found": [],
                    "improvements": [],
                    "source": "vector_db"
                }

                st.session_state.board_posts.append(board_post)
                moved_count += 1

        return {
            "success": True,
            "message": f"{moved_count}ê°œ ë¬¸ì„œ ì´ë™ ì™„ë£Œ",
            "count": moved_count
        }

    def get_knowledge_stats(self) -> Dict:
        """ì§€ì‹ ê´€ë¦¬ í†µê³„"""
        vector_db = st.session_state.get('vector_db', [])
        board_posts = st.session_state.get('board_posts', [])

        # í‰ê·  í’ˆì§ˆ ì ìˆ˜
        all_scores = []
        for doc in vector_db:
            if 'quality_score' in doc:
                all_scores.append(doc['quality_score'])
        for post in board_posts:
            if 'quality_score' in post:
                all_scores.append(post['quality_score'])

        avg_quality = sum(all_scores) / len(all_scores) if all_scores else 0

        # ì˜¤ëŠ˜ ë“±ë¡ëœ ë¬¸ì„œ ìˆ˜
        today = datetime.now().date()
        today_count = 0

        for doc in vector_db:
            if doc.get('timestamp') and doc['timestamp'].date() == today:
                today_count += 1

        for post in board_posts:
            if post.get('timestamp') and post['timestamp'].date() == today:
                today_count += 1

        return {
            'vector_count': len(vector_db),
            'board_count': len(board_posts),
            'avg_quality': avg_quality,
            'today_count': today_count,
            'total_documents': len(vector_db) + len(board_posts)
        }

    def get_file_info(self, uploaded_file) -> Dict:
        """íŒŒì¼ ì •ë³´ ë°˜í™˜"""
        return self.file_processor.get_file_info(uploaded_file)

    def search_knowledge(self, query: str) -> Dict:
        """ì§€ì‹ ê²€ìƒ‰"""
        vector_db = st.session_state.get('vector_db', [])
        board_posts = st.session_state.get('board_posts', [])

        results = {
            'vector_results': [],
            'board_results': [],
            'total_count': 0
        }

        query_lower = query.lower()

        # VectorDB ê²€ìƒ‰
        for doc in vector_db:
            if query_lower in doc.get('content', '').lower() or query_lower in doc.get('filename', '').lower():
                results['vector_results'].append(doc)

        # ê²Œì‹œíŒ ê²€ìƒ‰
        for post in board_posts:
            if query_lower in post.get('content', '').lower() or query_lower in post.get('title', '').lower():
                results['board_results'].append(post)

        results['total_count'] = len(results['vector_results']) + len(results['board_results'])
        return results