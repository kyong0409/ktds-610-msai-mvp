"""
LangGraph ê¸°ë°˜ Multi-Agent ì§€ì‹ ì°½ì¶œ ì—”ì§„
"""
from __future__ import annotations
from typing import TypedDict, List, Optional, Literal, Dict, Any, Tuple
import os
import json
import hashlib
import numpy as np
from datetime import datetime
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
try:
    from langchain_chroma import Chroma
except ImportError:
    from langchain_community.vectorstores import Chroma
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv

load_dotenv()

# Azure OpenAI ì„¤ì •
endpoint = os.environ["AZURE_ENDPOINT"]
api_key = os.environ["AZURE_AI_FOUNDRY_KEY"]
api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-12-01-preview")
embedding_api_version = os.getenv("AZURE_OPENAI_EMBEDDING_API_VERSION", "2024-12-01-preview")
embedding_deployment = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-3-large")
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4.1-mini")
GPT4O_DEPLOYMENT = os.getenv("AZURE_GPT4O_DEPLOYMENT", "gpt-4o")

# ì—­í• ë³„ LLM ì„¤ì • (ê¸°ë³¸ê°’)
DEFAULT_ROLES = {
    "librarian": {"model": DEPLOYMENT_NAME, "temperature": 0.0},
    "summarizer": {"model": DEPLOYMENT_NAME, "temperature": 0.0},
    "synthesizer": {"model": GPT4O_DEPLOYMENT, "temperature": 0.3},
    "verifier": {"model": DEPLOYMENT_NAME, "temperature": 0.0},
    "productizer": {"model": DEPLOYMENT_NAME, "temperature": 0.1},
}

# ì„œë¹„ìŠ¤ ì„¤ì • (ê¸°ë³¸ê°’)
DEFAULT_SERVICES = {
    "chroma": {
        "path": "./data/chroma_db",
        "collection": "knowledge_base",
        "hnsw_space": "cosine"
    },
    "sampling": {
        "k": 10,
        "quality_min": 0.6,
        "max_domain_per_sample": 2,
        "max_avg_sim": 0.30,
        "lambda_div": 0.7,
        "prefetch": 5000,
        "prekeep": 1200
    },
    "rag": {
        "top_k": 6
    }
}


# ë°ì´í„° ìŠ¤í‚¤ë§ˆ
class Chunk(TypedDict, total=False):
    doc_id: str
    chunk_id: str
    text: str
    domain: Optional[str]
    date: Optional[str]
    quality: float
    embedding: List[float]
    pii_flag: bool


class Summary(TypedDict):
    doc_id: str
    chunk_id: str
    problem: str
    constraints: List[str]
    approach: str
    outcomes: List[str]
    signals: List[str]
    risks: List[str]


class Proposal(TypedDict):
    kind: Literal["analogical", "pattern", "bridging"]
    statement: str
    applicability: Dict[str, List[str]]
    expected_effects: Dict[str, Any]
    risks_limits: List[str]
    evidence: List[Dict[str, Any]]
    quick_experiment: Dict[str, Any]


class Verdict(TypedDict):
    verdict: Literal["accept", "revise", "reject"]
    reasons: List[str]
    added_evidence: List[Dict[str, Any]]


class KNote(TypedDict):
    k_note_id: str
    title: str
    proposal: str
    applicability: Dict[str, List[str]]
    evidence: List[Dict[str, Any]]
    metrics_effect: Dict[str, Any]
    risks_limits: List[str]
    recommended_experiments: List[Dict[str, Any]]
    status: Literal["validated", "draft"]
    owners: List[str]
    version: str
    related: List[str]


class State(TypedDict, total=False):
    iter: int
    max_iter: int
    cfg_roles: Dict[str, Any]
    cfg_services: Dict[str, Any]
    chroma_collection: Any
    all_chunks_meta: List[Chunk]
    samples: List[Chunk]
    summaries: List[Summary]
    expansions: Dict[str, List[Chunk]]
    proposals: List[Proposal]
    verdicts: List[Verdict]
    knotes: List[KNote]
    scores: Dict[str, float]
    stop_reason: Optional[str]
    current_stage: str
    stages_completed: List[str]
    is_running: bool
    streamlit_state: Any
    quality_threshold: float
    enable_verification: bool


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def update_streamlit_state(state: State, stage: str, extra_data: Dict = None):
    """Streamlit ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° UI ë°˜ì˜"""
    st_state = state.get("streamlit_state")
    if st_state is not None and hasattr(st_state, 'creation_state'):
        update_dict = {
            "current_stage": stage,
            "stages_completed": state.get("stages_completed", []),
            "iteration": state.get("iter", 0),
            "max_iterations": state.get("max_iter", 3),
            "is_running": True
        }

        # ì¶”ê°€ ë°ì´í„° ë³‘í•©
        if extra_data:
            update_dict.update(extra_data)

        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st_state.creation_state.update(update_dict)

        # UI ì—…ë°ì´íŠ¸ (placeholderê°€ ìˆëŠ” ê²½ìš°)
        if hasattr(st_state, 'stage_placeholders') and stage in st_state.stage_placeholders:
            stage_names = {
                "normalize": "ğŸ“š ë°ì´í„° ì •ê·œí™”",
                "sample": "ğŸ² ë‹¤ì–‘ì„± ìƒ˜í”Œë§",
                "summarize": "ğŸ“ êµ¬ì¡°í™” ìš”ì•½",
                "expand": "ğŸ” RAG ì»¨í…ìŠ¤íŠ¸ í™•ì¥",
                "synthesize": "ğŸ§¬ ì•„ë‚ ë¡œì§€ ì œì•ˆ ìƒì„±",
                "verify": "âœ… ì œì•ˆ ê²€ì¦",
                "productize": "ğŸ“‹ K-Note ìƒì„±",
                "score": "ğŸ“Š í’ˆì§ˆ í‰ê°€"
            }
            st_state.stage_placeholders[stage].success(f"âœ… {stage_names.get(stage, stage)} - ì™„ë£Œ")


def update_stage_ui(state: State, stage: str, status: str = "running"):
    """ë‹¨ê³„ë³„ UI ìƒíƒœ ì—…ë°ì´íŠ¸ (running/completed)"""
    st_state = state.get("streamlit_state")
    if st_state is not None and hasattr(st_state, 'stage_placeholders') and stage in st_state.stage_placeholders:
        stage_names = {
            "normalize": "ğŸ“š ë°ì´í„° ì •ê·œí™”",
            "sample": "ğŸ² ë‹¤ì–‘ì„± ìƒ˜í”Œë§",
            "summarize": "ğŸ“ êµ¬ì¡°í™” ìš”ì•½",
            "expand": "ğŸ” RAG ì»¨í…ìŠ¤íŠ¸ í™•ì¥",
            "synthesize": "ğŸ§¬ ì•„ë‚ ë¡œì§€ ì œì•ˆ ìƒì„±",
            "verify": "âœ… ì œì•ˆ ê²€ì¦",
            "productize": "ğŸ“‹ K-Note ìƒì„±",
            "score": "ğŸ“Š í’ˆì§ˆ í‰ê°€"
        }

        if status == "running":
            st_state.stage_placeholders[stage].warning(f"ğŸ”„ {stage_names.get(stage, stage)} - ì§„í–‰ ì¤‘...")
        elif status == "completed":
            st_state.stage_placeholders[stage].success(f"âœ… {stage_names.get(stage, stage)} - ì™„ë£Œ")


def log_to_streamlit(state: State, message: str, level: str = "info"):
    """Streamlit í™”ë©´ì— ë¡œê·¸ ì‹¤ì‹œê°„ ì¶œë ¥"""
    st_state = state.get("streamlit_state")
    if st_state is not None:
        if not hasattr(st_state, 'creation_logs'):
            st_state.creation_logs = []

        from datetime import datetime
        timestamp = datetime.now().strftime("%H:%M:%S")
        st_state.creation_logs.append({"time": timestamp, "message": message, "level": level})

        # ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸
        if hasattr(st_state, 'log_placeholder'):
            with st_state.log_placeholder.container():
                # ìµœê·¼ 10ê°œ ë¡œê·¸ë§Œ í‘œì‹œ
                recent_logs = st_state.creation_logs[-10:]
                for log in recent_logs:
                    if log['level'] == 'error':
                        import streamlit as st
                        st.error(f"[{log['time']}] {log['message']}")
                    elif log['level'] == 'warning':
                        import streamlit as st
                        st.warning(f"[{log['time']}] {log['message']}")
                    elif log['level'] == 'success':
                        import streamlit as st
                        st.success(f"[{log['time']}] {log['message']}")
                    else:
                        import streamlit as st
                        st.info(f"[{log['time']}] {log['message']}")


def cosine(a: np.ndarray, b: np.ndarray) -> float:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì•ˆì „í•œ ë²„ì „)"""
    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    if not isinstance(a, np.ndarray):
        a = np.array(a, dtype=np.float32)
    if not isinstance(b, np.ndarray):
        b = np.array(b, dtype=np.float32)

    na = float(np.linalg.norm(a))
    nb = float(np.linalg.norm(b))

    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    if na < 1e-9 or nb < 1e-9:
        return 0.0

    dot_product = float(np.dot(a, b))
    return dot_product / (na * nb)


def z_norm(values: List[float]) -> List[float]:
    if not values:
        return values
    m, s = float(np.mean(values)), float(np.std(values) + 1e-9)
    return [(v - m) / s for v in values]


def recency_score(iso_dates: List[Optional[str]]) -> List[float]:
    now = datetime.utcnow()
    days = []
    for d in iso_dates:
        try:
            days.append((now - datetime.fromisoformat(d)).days)
        except:
            days.append(None)
    known = [x for x in days if x is not None]
    if not known:
        return [0.0] * len(days)
    mx = max(known) or 1
    raw = [1 - (x / mx) if x is not None else 0.5 for x in days]
    return z_norm(raw)


def diverse_select(
    embeddings: List[List[float]],
    qualities: List[float],
    recencies: List[float],
    domains: List[str],
    k: int,
    lambda_div: float = 0.7,
    max_per_domain: int = 2,
    max_avg_sim: float = 0.30,
) -> List[int]:
    n = len(embeddings)
    if n == 0:
        return []

    # ê¸¸ì´ ê²€ì¦
    if len(qualities) != n or len(recencies) != n or len(domains) != n:
        # ìµœì†Œ ê¸¸ì´ë¡œ ë§ì¶¤
        min_len = min(n, len(qualities), len(recencies), len(domains))
        embeddings = embeddings[:min_len]
        qualities = qualities[:min_len]
        recencies = recencies[:min_len]
        domains = domains[:min_len]
        n = min_len

        if n == 0:
            return []

    E = [np.array(e, dtype=np.float32) for e in embeddings]
    Q = z_norm(qualities)  # ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    R = list(recencies)     # ëª…ì‹œì ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
    S: List[int] = []
    dom_count: Dict[str, int] = {}

    # Seed ì„ íƒ
    base_scores = [0.5 * float(Q[i]) + 0.5 * float(R[i]) for i in range(n)]
    seed = int(np.argmax(base_scores))
    S.append(seed)
    dom_count[domains[seed]] = 1
    cand = set(range(n)) - set(S)

    attempts = 0
    max_attempts = n * 3  # ë¬´í•œ ë£¨í”„ ë°©ì§€

    while len(S) < min(k, n) and cand and attempts < max_attempts:
        attempts += 1
        scored: List[Tuple[float, int]] = []

        for i in list(cand):
            if dom_count.get(domains[i], 0) >= max_per_domain:
                continue
            max_sim = max(cosine(E[i], E[j]) for j in S)
            diversity = 1.0 - max_sim
            relevance = 0.5 * float(Q[i]) + 0.5 * float(R[i])
            score = lambda_div * diversity + (1 - lambda_div) * relevance
            scored.append((float(score), i))

        if not scored:
            break

        scored.sort(key=lambda x: x[0], reverse=True)
        chosen = scored[0][1]
        S.append(chosen)
        dom_count[domains[chosen]] = dom_count.get(domains[chosen], 0) + 1
        cand.remove(chosen)

        # í‰ê·  ìœ ì‚¬ë„ ì œì•½ (ë” ê´€ëŒ€í•˜ê²Œ ì ìš©)
        if len(S) >= 3:  # ìµœì†Œ 3ê°œëŠ” í™•ë³´
            sims = []
            for a in range(len(S)):
                for b in range(a + 1, len(S)):
                    sims.append(cosine(E[S[a]], E[S[b]]))

            avg_sim = float(np.mean(sims)) if sims else 0.0

            # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ ì œê±° (í•˜ì§€ë§Œ ìµœì†Œ 3ê°œëŠ” ìœ ì§€)
            if avg_sim > max_avg_sim and len(S) > 3:
                dom_count[domains[chosen]] -= 1
                S.pop()
                continue

    return S


def hash_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


# LLM íŒ©í† ë¦¬
def get_llm(role: str, cfg_roles: Dict[str, Any]):
    r = cfg_roles[role]
    return AzureChatOpenAI(
        azure_endpoint=endpoint,
        api_key=api_key,
        api_version=api_version,
        azure_deployment=r["model"],
        temperature=r["temperature"],
        max_retries=3,
        timeout=30
    )


# Chroma ê´€ë ¨ í•¨ìˆ˜
def ensure_chroma(state: State, chroma_persist_directory: str, collection_name: str) -> None:
    if state.get("chroma_collection"):
        return

    embeddings = AzureOpenAIEmbeddings(
        azure_endpoint=endpoint,
        api_key=api_key,
        api_version=embedding_api_version,
        azure_deployment=embedding_deployment
    )

    try:
        vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=embeddings,
            persist_directory=chroma_persist_directory
        )
    except KeyError as e:
        if "_type" in str(e):
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì— ë¬¸ì œê°€ ìˆìœ¼ë©´ ì‚­ì œí•˜ê³  ì¬ìƒì„±
            import chromadb
            from chromadb.config import Settings

            client = chromadb.PersistentClient(
                path=chroma_persist_directory,
                settings=Settings(anonymized_telemetry=False)
            )

            try:
                # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
                client.delete_collection(name=collection_name)
            except Exception as del_e:
                pass

            # ìƒˆë¡œ ìƒì„±
            vectorstore = Chroma(
                collection_name=collection_name,
                embedding_function=embeddings,
                persist_directory=chroma_persist_directory
            )
        else:
            raise

    state["chroma_collection"] = vectorstore


def chroma_prefetch_candidates(collection, quality_min=0.6, prefetch=100, prekeep=50) -> Dict[str, List]:
    """
    ChromaDBì—ì„œ í›„ë³´ ì²­í¬ ì¶”ì¶œ (ìƒ˜í”Œë§)
    """
    try:
        # Chroma Langchain wrapperì—ì„œ ë‚´ë¶€ ì»¬ë ‰ì…˜ ì ‘ê·¼
        if hasattr(collection, '_collection'):
            chroma_collection = collection._collection
        else:
            chroma_collection = collection

        # ì „ì²´ ë¬¸ì„œ ì¡°íšŒ
        results = chroma_collection.get(
            include=["documents", "embeddings", "metadatas"],
            limit=prefetch if prefetch else None
        )

        if not results["ids"] or len(results["ids"]) == 0:
            return {"ids": [], "documents": [], "embeddings": [], "metadatas": []}

        # ì„ë² ë”© ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        has_embeddings = results.get("embeddings") is not None and len(results.get("embeddings", [])) > 0

        # í’ˆì§ˆ í•„í„°ë§
        filtered = {"ids": [], "documents": [], "embeddings": [], "metadatas": []}
        for i, metadata in enumerate(results["metadatas"]):
            if metadata is None:
                metadata = {}

            quality = metadata.get("quality_score", 0)
            if isinstance(quality, str):
                try:
                    quality = float(quality)
                except:
                    quality = 0

            quality = quality / 100.0 if quality > 1 else quality  # 0~100 -> 0~1

            if quality >= quality_min:
                filtered["ids"].append(results["ids"][i])
                filtered["documents"].append(results["documents"][i])

                # ì„ë² ë”© ì²˜ë¦¬ (Noneì¼ ìˆ˜ ìˆìŒ)
                if has_embeddings and i < len(results["embeddings"]):
                    emb = results["embeddings"][i]
                    filtered["embeddings"].append(emb if emb is not None else [])
                else:
                    filtered["embeddings"].append([])

                filtered["metadatas"].append(metadata)

        # ëœë¤ ìƒ˜í”Œë§
        n = len(filtered["ids"])
        if n > prekeep:
            idx = np.random.choice(n, size=prekeep, replace=False)
            for k in filtered.keys():
                filtered[k] = [filtered[k][int(i)] for i in idx]

        return filtered

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"ids": [], "documents": [], "embeddings": [], "metadatas": []}


def chroma_query_texts(collection, query_texts: List[str], top_k: int = 6) -> List[Chunk]:
    """
    Chroma ë²¡í„° ê²€ìƒ‰ (RAGìš©)
    """
    try:
        # query_textsë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ë¦¬ìŠ¤íŠ¸ë‚˜ ë‹¤ë¥¸ íƒ€ì…ì´ í¬í•¨ë  ìˆ˜ ìˆìŒ)
        clean_texts = []
        for item in query_texts:
            if isinstance(item, str):
                clean_texts.append(item)
            elif isinstance(item, list):
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
                clean_texts.extend([str(x) for x in item if x])
            else:
                clean_texts.append(str(item))

        query_str = " ".join(clean_texts)
        results = collection.similarity_search(query_str, k=top_k)

        out: List[Chunk] = []
        for doc in results:
            md = doc.metadata or {}
            out.append({
                "doc_id": md.get("doc_id", "unknown"),
                "chunk_id": md.get("chunk_id", "unknown"),
                "text": doc.page_content,
                "embedding": [],
                "domain": md.get("domain", "unknown"),
                "date": md.get("date"),
                "quality": md.get("quality_score", 0.5) / 100.0,
                "pii_flag": md.get("pii_flag", False),
            })

        return out

    except Exception as e:
        import traceback
        traceback.print_exc()
        return []


# LangGraph ë…¸ë“œ
def node_normalize(state: State) -> State:
    """1ë‹¨ê³„: ë°ì´í„° ì •ê·œí™” ë° ìƒ˜í”Œ í›„ë³´ ìˆ˜ì§‘"""
    state["current_stage"] = "normalize"
    update_stage_ui(state, "normalize", "running")
    log_to_streamlit(state, "ğŸ“š [1/8] ë°ì´í„° ì •ê·œí™” ì‹œì‘", "info")

    s = state["cfg_services"]["sampling"]
    chroma_cfg = state["cfg_services"]["chroma"]

    log_to_streamlit(state, "  â†’ ChromaDB ì—°ê²° ì¤‘...", "info")
    ensure_chroma(state, chroma_cfg["path"], chroma_cfg["collection"])

    log_to_streamlit(state, f"  â†’ í›„ë³´ ì²­í¬ ìˆ˜ì§‘ ì¤‘ (í’ˆì§ˆ ê¸°ì¤€: {s['quality_min']})", "info")
    batch = chroma_prefetch_candidates(
        state["chroma_collection"],
        quality_min=s["quality_min"],
        prefetch=s.get("prefetch", 100),
        prekeep=s.get("prekeep", 50)
    )

    state["all_chunks_meta"] = []
    for i, _id in enumerate(batch["ids"]):
        md = batch["metadatas"][i] or {}
        state["all_chunks_meta"].append({
            "doc_id": md.get("doc_id") or _id.split("::")[0],
            "chunk_id": md.get("chunk_id") or str(i),
            "text": batch["documents"][i],
            "domain": md.get("domain", "unknown"),
            "date": md.get("date"),
            "quality": md.get("quality_score", 50) / 100.0,
            "embedding": batch["embeddings"][i] if batch["embeddings"] else [],
            "pii_flag": md.get("pii_flag", False),
        })

    log_to_streamlit(state, f"  âœ… ì •ê·œí™” ì™„ë£Œ: {len(state['all_chunks_meta'])}ê°œ ì²­í¬ ìˆ˜ì§‘", "success")
    state["stages_completed"].append("normalize")
    update_stage_ui(state, "normalize", "completed")
    return state


def node_sample(state: State) -> State:
    """2ë‹¨ê³„: MMR ê¸°ë°˜ ë‹¤ì–‘ì„± ìƒ˜í”Œë§"""
    state["current_stage"] = "sample"
    update_stage_ui(state, "sample", "running")
    log_to_streamlit(state, "ğŸ² [2/8] ë‹¤ì–‘ì„± ìƒ˜í”Œë§ ì‹œì‘", "info")

    try:
        s = state["cfg_services"]["sampling"]
        pool = [c for c in state["all_chunks_meta"]
                if c["quality"] >= s["quality_min"] and not c.get("pii_flag", False)]

        log_to_streamlit(state, f"  â†’ ì „ì²´ {len(state['all_chunks_meta'])}ê°œ ì¤‘ {len(pool)}ê°œê°€ í’ˆì§ˆ ê¸°ì¤€ í†µê³¼", "info")

        if not pool:
            log_to_streamlit(state, "  âš ï¸ í’ˆì§ˆ ê¸°ì¤€ì„ í†µê³¼í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤", "warning")
            state["samples"] = []
            state["stages_completed"].append("sample")
            return state

        # ì„ë² ë”©ì´ ìˆëŠ” ì²­í¬ë§Œ í•„í„°ë§
        pool_with_embeddings = []
        for c in pool:
            emb = c.get("embedding")
            # ë¦¬ìŠ¤íŠ¸ì´ê³  ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
            if emb is not None and isinstance(emb, (list, np.ndarray)) and len(emb) > 0:
                pool_with_embeddings.append(c)

        log_to_streamlit(state, f"  â†’ ì„ë² ë”©ì´ ìˆëŠ” ì²­í¬: {len(pool_with_embeddings)}ê°œ", "info")

        if not pool_with_embeddings:
            # ì„ë² ë”©ì´ ì—†ìœ¼ë©´ ëœë¤ ìƒ˜í”Œ
            log_to_streamlit(state, "  â†’ ì„ë² ë”© ì—†ìŒ, ëœë¤ ìƒ˜í”Œë§ ìˆ˜í–‰", "warning")
            import random
            state["samples"] = random.sample(pool, min(s["k"], len(pool)))
            state["stages_completed"].append("sample")
            return state

        # ì„ë² ë”© ìˆëŠ” ì²­í¬ë“¤ì— ëŒ€í•´ì„œë§Œ ì²˜ë¦¬
        embeddings = [c["embedding"] for c in pool_with_embeddings]
        qualities = [c["quality"] for c in pool_with_embeddings]
        recencies = recency_score([c.get("date") for c in pool_with_embeddings])
        domains = [c.get("domain", "unknown") for c in pool_with_embeddings]

        log_to_streamlit(state, f"  â†’ MMR ë‹¤ì–‘ì„± ìƒ˜í”Œë§ ì‹¤í–‰ ì¤‘ (ëª©í‘œ: {s['k']}ê°œ)...", "info")

        idxs = diverse_select(
            embeddings, qualities, recencies, domains,
            k=s["k"],
            lambda_div=s["lambda_div"],
            max_per_domain=s["max_domain_per_sample"],
            max_avg_sim=s["max_avg_sim"]
        )

        state["samples"] = [pool_with_embeddings[i] for i in idxs]
        log_to_streamlit(state, f"  âœ… ìƒ˜í”Œë§ ì™„ë£Œ: {len(state['samples'])}ê°œ ì„ íƒ", "success")
        state["stages_completed"].append("sample")
        update_stage_ui(state, "sample", "completed")
        return state

    except Exception as e:
        log_to_streamlit(state, f"  âŒ ìƒ˜í”Œë§ ì˜¤ë¥˜: {str(e)}", "error")
        import traceback
        traceback.print_exc()
        # í´ë°±: ëœë¤ ìƒ˜í”Œë§
        import random
        pool = state.get("all_chunks_meta", [])
        state["samples"] = random.sample(pool, min(10, len(pool))) if pool else []
        state["stages_completed"].append("sample")
        update_stage_ui(state, "sample", "completed")
        return state


def node_summarize(state: State) -> State:
    """3ë‹¨ê³„: êµ¬ì¡°í™” ìš”ì•½ ìƒì„±"""
    state["current_stage"] = "summarize"
    update_stage_ui(state, "summarize", "running")
    log_to_streamlit(state, f"ğŸ“ [3/8] êµ¬ì¡°í™” ìš”ì•½ ìƒì„± ì‹œì‘ (ìƒ˜í”Œ: {len(state['samples'])}ê°œ)", "info")

    llm = get_llm("summarizer", state["cfg_roles"])
    outs: List[Summary] = []

    for idx, c in enumerate(state["samples"], 1):
        log_to_streamlit(state, f"  â†’ ìš”ì•½ ìƒì„± ì¤‘ ({idx}/{len(state['samples'])}): {c['doc_id'][:20]}...", "info")

        prompt = f"""
ì•„ë˜ í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ê¸°ìˆ  ë¬¸ì„œ êµ¬ì¡° ìš”ì•½(JSON) ìƒì„±.
í•„ë“œ: problem, constraints[], approach, outcomes[], signals[], risks[].
í…ìŠ¤íŠ¸: ```{c['text'][:1000]}```
JSONë§Œ ì¶œë ¥.
"""
        try:
            response = llm.invoke(prompt).content
            # JSON íŒŒì‹±
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response

            s: Summary = json.loads(json_str)
            s["doc_id"] = c["doc_id"]
            s["chunk_id"] = c["chunk_id"]
            outs.append(s)
        except Exception as e:
            log_to_streamlit(state, f"    âš ï¸ ìš”ì•½ ì‹¤íŒ¨: {str(e)[:50]}", "warning")
            # ê¸°ë³¸ ìš”ì•½
            outs.append({
                "doc_id": c["doc_id"],
                "chunk_id": c["chunk_id"],
                "problem": "ë¶„ì„ ì¤‘",
                "constraints": [],
                "approach": "",
                "outcomes": [],
                "signals": [],
                "risks": []
            })

    log_to_streamlit(state, f"  âœ… ìš”ì•½ ë‹¨ê³„ ì™„ë£Œ: {len(outs)}ê°œ ìƒì„±", "success")
    state["summaries"] = outs
    state["stages_completed"].append("summarize")
    update_stage_ui(state, "summarize", "completed")
    return state


def node_expand_rag(state: State) -> State:
    """4ë‹¨ê³„: RAG ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ í™•ì¥"""
    state["current_stage"] = "expand"
    update_stage_ui(state, "expand", "running")
    log_to_streamlit(state, f"ğŸ” [4/8] RAG ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ì‹œì‘ (ìš”ì•½: {len(state['summaries'])}ê°œ)", "info")

    col = state["chroma_collection"]
    top_k = state["cfg_services"]["rag"]["top_k"]
    expansions: Dict[str, List[Chunk]] = {}

    for idx, s in enumerate(state["summaries"], 1):
        queries = []
        if s["problem"]:
            queries.append(s["problem"])
        if s["approach"]:
            queries.append(s["approach"])
        for con in s["constraints"][:2]:
            queries.append(con)

        if not queries:
            queries = ["general engineering patterns"]

        ctx = chroma_query_texts(col, queries, top_k=top_k)
        expansions[f"{s['doc_id']}::{s['chunk_id']}"] = ctx

    total_ctx = sum(len(v) for v in expansions.values())
    log_to_streamlit(state, f"  âœ… RAG í™•ì¥ ì™„ë£Œ: ì´ {total_ctx}ê°œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘", "success")
    state["expansions"] = expansions
    state["stages_completed"].append("expand")
    update_stage_ui(state, "expand", "completed")
    return state


def node_synthesize(state: State) -> State:
    """5ë‹¨ê³„: ì•„ë‚ ë¡œì§€ ê¸°ë°˜ ì œì•ˆ ìƒì„±"""
    state["current_stage"] = "synthesize"
    update_stage_ui(state, "synthesize", "running")
    log_to_streamlit(state, f"ğŸ§¬ [5/8] ì•„ë‚ ë¡œì§€ ê¸°ë°˜ ìœµí•© ì œì•ˆ ìƒì„± ì‹œì‘", "info")

    llm = get_llm("synthesizer", state["cfg_roles"])
    props: List[Proposal] = []

    # í˜ì–´ë§
    pairs = list(zip(state["summaries"][::2], state["summaries"][1::2]))
    log_to_streamlit(state, f"  â†’ ìš”ì•½ í˜ì–´ë§: {len(pairs)}ìŒ ìƒì„±", "info")

    for a, b in pairs:
        key_a = f"{a['doc_id']}::{a['chunk_id']}"
        key_b = f"{b['doc_id']}::{b['chunk_id']}"
        ctx_a = state["expansions"].get(key_a, [])
        ctx_b = state["expansions"].get(key_b, [])

        # ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        ctx_a_data = [{'doc_id': x['doc_id'], 'text': x['text'][:200]} for x in ctx_a[:2]]
        ctx_b_data = [{'doc_id': x['doc_id'], 'text': x['text'][:200]} for x in ctx_b[:2]]

        prompt = f"""
[ì—­í• ] ì•„ë‚ ë¡œì§€ ê¸°ë°˜ ì„¤ê³„ì
[ì…ë ¥]
A={json.dumps(a, ensure_ascii=False)}
B={json.dumps(b, ensure_ascii=False)}
A_ctx={json.dumps(ctx_a_data, ensure_ascii=False)}
B_ctx={json.dumps(ctx_b_data, ensure_ascii=False)}

[ì§€ì‹œ]
- Analogical/Pattern/Bridging ì œì•ˆ 1~2ê°œ ìƒì„±.
- ê° ì œì•ˆ: statement, applicability(when/when_not/assumptions), expected_effects, risks_limits, evidence[doc_id,chunk_id,quote,confidence], quick_experiment.
- JSON ë°°ì—´ë¡œë§Œ ì¶œë ¥.
"""
        try:
            response = llm.invoke(prompt).content
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response

            parsed = json.loads(json_str)
            new_props = parsed if isinstance(parsed, list) else [parsed]
            props.extend(new_props)
            log_to_streamlit(state, f"  â†’ í˜ì–´ {len(props)//2 + 1}/{len(pairs)}: {len(new_props)}ê°œ ì œì•ˆ ìƒì„±", "info")
        except Exception as e:
            log_to_streamlit(state, f"  âš ï¸ í•©ì„± ì‹¤íŒ¨: {str(e)[:50]}", "warning")

    log_to_streamlit(state, f"  âœ… ìœµí•© ì œì•ˆ ì™„ë£Œ: ì´ {len(props)}ê°œ ì œì•ˆ", "success")
    state["proposals"] = props
    state["stages_completed"].append("synthesize")
    update_stage_ui(state, "synthesize", "completed")
    return state


def node_verify(state: State) -> State:
    """6ë‹¨ê³„: ì œì•ˆ ê²€ì¦"""
    state["current_stage"] = "verify"
    update_stage_ui(state, "verify", "running")

    # ê²€ì¦ ë‹¨ê³„ í™œì„±í™” ì—¬ë¶€ í™•ì¸
    enable_verification = state.get("enable_verification", True)

    if not enable_verification:
        log_to_streamlit(state, f"â­ï¸ [6/8] ì œì•ˆ ê²€ì¦ ê±´ë„ˆëœ€ (ë¹„í™œì„±í™”ë¨)", "info")
        # ëª¨ë“  ì œì•ˆì„ ìë™ ìŠ¹ì¸
        verdicts: List[Verdict] = []
        for p in state["proposals"]:
            verdicts.append({"verdict": "accept", "reasons": ["ê²€ì¦ ë‹¨ê³„ ë¹„í™œì„±í™”"], "added_evidence": []})
        state["verdicts"] = verdicts
        state["stages_completed"].append("verify")
        update_stage_ui(state, "verify", "completed")
        return state

    log_to_streamlit(state, f"âœ… [6/8] ì œì•ˆ ê²€ì¦ ì‹œì‘ (ì œì•ˆ: {len(state['proposals'])}ê°œ)", "info")

    llm = get_llm("verifier", state["cfg_roles"])
    col = state["chroma_collection"]
    verdicts: List[Verdict] = []

    for idx, p in enumerate(state["proposals"], 1):
        q = [p["statement"]] + p["applicability"].get("assumptions", [])[:1]
        counter_ctx = chroma_query_texts(col, q, top_k=3)

        # ë°˜ë¡€ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ê³„ì‚°
        counter_data = [{'text': c['text'][:200]} for c in counter_ctx]

        prompt = f"""
[ì—­í• ] ê²€ì¦ì
proposal={json.dumps(p, ensure_ascii=False)}
counter_evidence={json.dumps(counter_data, ensure_ascii=False)}

[ì§€ì‹œ]
- ë°˜ë¡€/í¸í–¥/ì™¸ì‚½ ìœ„í—˜ í‰ê°€.
- verdict: accept|revise|reject
- reasons[], added_evidence[] JSONë§Œ ì¶œë ¥.
"""
        try:
            response = llm.invoke(prompt).content
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response

            verdict = json.loads(json_str)
            verdicts.append(verdict)
        except Exception as e:
            log_to_streamlit(state, f"  âš ï¸ ê²€ì¦ ì‹¤íŒ¨: {str(e)[:50]}", "warning")
            verdicts.append({"verdict": "reject", "reasons": ["ê²€ì¦ ì˜¤ë¥˜"], "added_evidence": []})

    accept_count = sum(1 for v in verdicts if v.get("verdict") == "accept")
    log_to_streamlit(state, f"  âœ… ê²€ì¦ ì™„ë£Œ: {accept_count}/{len(verdicts)}ê°œ ìŠ¹ì¸", "success")

    state["verdicts"] = verdicts
    state["stages_completed"].append("verify")
    update_stage_ui(state, "verify", "completed")
    return state


def node_productize(state: State) -> State:
    """7ë‹¨ê³„: K-Note ìƒì„±"""
    state["current_stage"] = "productize"
    update_stage_ui(state, "productize", "running")

    llm = get_llm("productizer", state["cfg_roles"])
    kns: List[KNote] = []

    accepted_proposals = [(p, v) for p, v in zip(state["proposals"], state["verdicts"])
                          if v["verdict"] == "accept"]
    log_to_streamlit(state, f"ğŸ“‹ [7/8] K-Note ìƒì„± ì‹œì‘ (ìŠ¹ì¸ëœ ì œì•ˆ: {len(accepted_proposals)}ê°œ)", "info")

    for idx, (p, v) in enumerate(accepted_proposals, 1):
        prompt = f"""
ì•„ë˜ proposalì„ K-Note ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜í•´ JSONë§Œ ì¶œë ¥.
í•„ìˆ˜: k_note_id(ì„ì‹œ), title, proposal, applicability, evidence, metrics_effect, risks_limits, recommended_experiments, status, owners, version, related
proposal={json.dumps(p, ensure_ascii=False)}
"""
        try:
            response = llm.invoke(prompt).content
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response

            kn = json.loads(json_str)
            kn["k_note_id"] = kn.get("k_note_id") or f"KN-{hash_text(p['statement'])[:8]}"
            kn["status"] = kn.get("status") or "validated"
            kns.append(kn)
            log_to_streamlit(state, f"  â†’ K-Note {idx}/{len(accepted_proposals)}: {kn.get('title', 'Untitled')[:40]}...", "info")
        except Exception as e:
            log_to_streamlit(state, f"  âš ï¸ K-Note ìƒì„± ì‹¤íŒ¨: {str(e)[:50]}", "warning")

    log_to_streamlit(state, f"  âœ… K-Note ìƒì„± ì™„ë£Œ: ì´ {len(kns)}ê°œ", "success")

    state["knotes"] = state.get("knotes", []) + kns
    state["stages_completed"].append("productize")
    update_stage_ui(state, "productize", "completed")
    return state


def node_score(state: State) -> State:
    """8ë‹¨ê³„: í‰ê°€"""
    state["current_stage"] = "score"
    update_stage_ui(state, "score", "running")
    log_to_streamlit(state, f"ğŸ“Š [8/8] í’ˆì§ˆ í‰ê°€ ì‹œì‘", "info")

    # ê°„ë‹¨ ì ìˆ˜ (ì‹¤ì œë¡œëŠ” ì„ë² ë”© ê¸°ë°˜ ì‹ ê·œì„±/ì»¤ë²„ë¦¬ì§€ í‰ê°€ ê°€ëŠ¥)
    import random
    state["scores"] = {
        "novelty": round(random.uniform(0.6, 0.9), 2),
        "coverage": round(random.uniform(0.6, 0.9), 2),
        "utility": round(random.uniform(0.6, 0.9), 2)
    }

    avg_score = np.mean(list(state["scores"].values()))
    log_to_streamlit(state, f"  â†’ ì‹ ê·œì„±: {state['scores']['novelty']}, ì»¤ë²„ë¦¬ì§€: {state['scores']['coverage']}, ìœ ìš©ì„±: {state['scores']['utility']}", "info")
    log_to_streamlit(state, f"  âœ… í’ˆì§ˆ í‰ê°€ ì™„ë£Œ (í‰ê· : {avg_score:.2f})", "success")

    state["stages_completed"].append("score")
    update_stage_ui(state, "score", "completed")
    return state


def should_continue(state: State) -> str:
    """ë°˜ë³µ ì¡°ê±´ í‰ê°€"""
    s = state.get("scores", {})
    avg = np.mean([s.get("novelty", 0), s.get("coverage", 0), s.get("utility", 0)])

    # ì‚¬ìš©ì ì„¤ì • í’ˆì§ˆ ì„ê³„ê°’ ì‚¬ìš©
    quality_threshold = state.get("quality_threshold", 0.75)

    log_to_streamlit(state, f"ğŸ”„ ë°˜ë³µ ì¡°ê±´ í‰ê°€: í‰ê·  ì ìˆ˜={avg:.2f}, ì„ê³„ê°’={quality_threshold:.2f}", "info")

    if avg >= quality_threshold:
        state["stop_reason"] = f"score_threshold({avg:.2f}>={quality_threshold:.2f})"
        state["is_running"] = False
        log_to_streamlit(state, f"âœ… í’ˆì§ˆ ì„ê³„ê°’ ë‹¬ì„±ìœ¼ë¡œ ì™„ë£Œ: {avg:.2f}>={quality_threshold:.2f}", "success")
        return "stop"

    # iter ì¹´ìš´í„° ì¦ê°€ë¥¼ ë¨¼ì € ìˆ˜í–‰
    current_iter = state.get("iter", 0) + 1
    state["iter"] = current_iter
    max_iter = state.get("max_iter", 3)

    log_to_streamlit(state, f"ğŸ”„ ë°˜ë³µ {current_iter}/{max_iter}", "info")

    if current_iter >= max_iter:
        state["stop_reason"] = "max_iter"
        state["is_running"] = False
        log_to_streamlit(state, f"âœ… ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬ë¡œ ì™„ë£Œ: {current_iter}/{max_iter}", "success")
        return "stop"

    state["stages_completed"] = ["normalize"]  # ì¼ë¶€ ë‹¨ê³„ ìœ ì§€
    log_to_streamlit(state, f"ğŸ”„ ë‹¤ìŒ ë°˜ë³µ ê³„ì† (ë°˜ë³µ {current_iter})", "info")
    return "continue"


# ê·¸ë˜í”„ ë¹Œë”
def build_graph() -> Any:
    """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    g = StateGraph(State)

    g.add_node("normalize", node_normalize)
    g.add_node("sample", node_sample)
    g.add_node("summarize", node_summarize)
    g.add_node("expand", node_expand_rag)
    g.add_node("synthesize", node_synthesize)
    g.add_node("verify", node_verify)
    g.add_node("productize", node_productize)
    g.add_node("score", node_score)

    g.add_edge("normalize", "sample")
    g.add_edge("sample", "summarize")
    g.add_edge("summarize", "expand")
    g.add_edge("expand", "synthesize")
    g.add_edge("synthesize", "verify")
    g.add_edge("verify", "productize")
    g.add_edge("productize", "score")
    g.add_conditional_edges("score", should_continue, {"continue": "sample", "stop": END})

    g.set_entry_point("normalize")

    return g


class KnowledgeCreationEngine:
    """ì§€ì‹ ì°½ì¶œ ì—”ì§„ ë˜í¼ í´ë˜ìŠ¤"""

    def __init__(self, chroma_persist_directory: str = "./data/chroma_db",
                 collection_name: str = "knowledge_base",
                 max_iterations: int = 3):
        self.chroma_persist_directory = chroma_persist_directory
        self.collection_name = collection_name
        self.max_iterations = max_iterations

        # LangGraph ì»´íŒŒì¼
        self.graph = build_graph().compile()

    def run(self, max_iter: int = None, streamlit_state = None,
            quality_threshold: float = 0.75, agent_temperature: float = 0.3,
            enable_verification: bool = True) -> State:
        """ì§€ì‹ ì°½ì¶œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        if max_iter is None:
            max_iter = self.max_iterations

        # ì—­í• ë³„ temperature ë™ì  ì„¤ì •
        roles_config = {
            "librarian": {"model": DEPLOYMENT_NAME, "temperature": 0.0},  # ì •ê·œí™”ëŠ” í•­ìƒ 0.0
            "summarizer": {"model": DEPLOYMENT_NAME, "temperature": 0.0},  # ìš”ì•½ë„ ì •í™•ì„± ìœ„í•´ 0.0
            "synthesizer": {"model": GPT4O_DEPLOYMENT, "temperature": agent_temperature},  # ì°½ì˜ì„± ì ìš©
            "verifier": {"model": DEPLOYMENT_NAME, "temperature": 0.0},  # ê²€ì¦ì€ í•­ìƒ 0.0
            "productizer": {"model": DEPLOYMENT_NAME, "temperature": agent_temperature * 0.3},  # ì•½ê°„ì˜ ì°½ì˜ì„±
        }

        # ì´ˆê¸° ìƒíƒœ
        initial_state: State = {
            "iter": 0,
            "max_iter": max_iter,
            "cfg_roles": roles_config,
            "cfg_services": {
                **DEFAULT_SERVICES,
                "chroma": {
                    "path": self.chroma_persist_directory,
                    "collection": self.collection_name,
                    "hnsw_space": "cosine"
                }
            },
            "knotes": [],
            "stages_completed": [],
            "current_stage": "normalize",
            "is_running": True,
            "quality_threshold": quality_threshold,
            "enable_verification": enable_verification
        }

        # Streamlit ì„¸ì…˜ ìƒíƒœ ì—°ê²°
        if streamlit_state is not None:
            initial_state["streamlit_state"] = streamlit_state

        # ê·¸ë˜í”„ ì‹¤í–‰ (recursion_limit ì„¤ì •)
        result = self.graph.invoke(initial_state, config={"recursion_limit": 100})

        return result