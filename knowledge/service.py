"""
ì§€ì‹ ê´€ë¦¬ ì„œë¹„ìŠ¤ ë¡œì§
"""
import streamlit as st
import time
from datetime import datetime
from typing import Dict, Any, Tuple, Optional
from utils.file_processor import FileProcessor
from markitdown import MarkItDown
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from dotenv import load_dotenv
import os
from azure.core.credentials import AzureKeyCredential
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.text_splitter import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
    TokenTextSplitter
)
from langchain_experimental.text_splitter import SemanticChunker
try:
    from langchain_chroma import Chroma
except ImportError:
    from langchain_community.vectorstores import Chroma
import sqlite3
import uuid
from typing import List

from knowledge.prompts import (
    ANALYZE_DOCUMENT_PROMPT,
    GENERATE_ENHANCED_KNOWLEDGE_DOCUMENT_PROMPT,
    KNOTE_TO_STANDARD_DOC_PROMPT,
)

load_dotenv()

AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
AZURE_DOCUMENT_INTELLIGENCE_KEY = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_KEY")

key = AzureKeyCredential(AZURE_DOCUMENT_INTELLIGENCE_KEY)

# Azure OpenAI ì„¤ì •
endpoint = os.environ["AZURE_ENDPOINT"]
api_key = os.environ["AZURE_AI_FOUNDRY_KEY"]
api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-12-01-preview")
embedding_api_version = os.getenv("AZURE_OPENAI_EMBEDDING_API_VERSION", "2024-12-01-preview")
embedding_deployment = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-3-large")

print("endpoint: ", endpoint)
print("api_key: ", api_key)
print("api_version: ", api_version)

# ë°°í¬ ì´ë¦„: Foundryì—ì„œ ë§Œë“  ë°°í¬ëª…ê³¼ ë™ì¼í•´ì•¼ í•¨
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4.1-mini")
print("DEPLOYMENT_NAME: ", DEPLOYMENT_NAME)
llm = AzureChatOpenAI(
		azure_endpoint=endpoint,
		api_key=api_key,
		api_version=api_version,
		azure_deployment=DEPLOYMENT_NAME,
		temperature=0.4,
		max_retries=3,
		timeout=30,
)

class KnowledgeService:
    """ì§€ì‹ ê´€ë¦¬ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        self.file_processor = FileProcessor()

        # MarkItDown ì´ˆê¸°í™” (Azure Document Intelligence ì„ íƒì  ì‚¬ìš©)
        try:
            if AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT and AZURE_DOCUMENT_INTELLIGENCE_KEY:
                print(f"[INFO] MarkItDown with Azure Document Intelligence ì´ˆê¸°í™” ì¤‘...")
                self.markitdown = MarkItDown(
                    docintel_endpoint=AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT,
                    docintel_credential=key
                )
                print(f"[INFO] MarkItDown ì´ˆê¸°í™” ì„±ê³µ (Azure DI í™œì„±í™”)")
            else:
                print(f"[WARNING] Azure Document Intelligence ì„¤ì • ëˆ„ë½, ê¸°ë³¸ MarkItDown ì‚¬ìš©")
                self.markitdown = MarkItDown()
                print(f"[INFO] MarkItDown ì´ˆê¸°í™” ì„±ê³µ (ê¸°ë³¸ ëª¨ë“œ)")
        except Exception as e:
            print(f"[ERROR] MarkItDown ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print(f"[INFO] Fallback: ê¸°ë³¸ MarkItDown ì‚¬ìš©")
            self.markitdown = MarkItDown()

    def convert_file_to_text(self, uploaded_file) -> Tuple[bool, str]:
        """MarkItDownì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            import tempfile
            import os

            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_file_path = tmp_file.name

            print(f"[DEBUG] MarkItDown ë³€í™˜ ì‹œë„: {tmp_file_path}")

            # MarkItDownìœ¼ë¡œ ë³€í™˜
            result = self.markitdown.convert(tmp_file_path, keep_data_uris=True)

            print(f"[DEBUG] MarkItDown ë³€í™˜ ì„±ê³µ")

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)

            # ì—…ë¡œë“œ íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            uploaded_file.seek(0)

            return True, result.text_content

        except Exception as e:
            print(f"[ERROR] MarkItDown ë³€í™˜ ì‹¤íŒ¨: {type(e).__name__}: {str(e)}")
            import traceback
            traceback.print_exc()

            # ì—…ë¡œë“œ íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            uploaded_file.seek(0)

            # Fallback: ê¸°ì¡´ íŒŒì¼ ì²˜ë¦¬ ë°©ì‹ ì‚¬ìš©
            print(f"[INFO] Fallback í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„")
            fallback_content = self._fallback_text_extraction(uploaded_file)
            if fallback_content:
                print(f"[INFO] Fallback í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ ({len(fallback_content)} chars)")
                return True, fallback_content

            return False, f"íŒŒì¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def _fallback_text_extraction(self, uploaded_file) -> str:
        """Fallback: ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ì‹"""
        try:
            # íŒŒì¼ íƒ€ì…ë³„ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            file_extension = uploaded_file.name.split('.')[-1].lower()

            if file_extension == 'txt':
                # í…ìŠ¤íŠ¸ íŒŒì¼ì€ ë°”ë¡œ ì½ê¸°
                content = uploaded_file.read().decode('utf-8', errors='ignore')
                return content
            elif file_extension == 'pdf':
                # PDFëŠ” ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜
                return f"PDF íŒŒì¼: {uploaded_file.name}\n(MarkItDown ë³€í™˜ ì‹¤íŒ¨, ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”)"
            elif file_extension in ['docx', 'doc']:
                # Word íŒŒì¼ì€ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜
                return f"Word ë¬¸ì„œ: {uploaded_file.name}\n(MarkItDown ë³€í™˜ ì‹¤íŒ¨, ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”)"
            elif file_extension in ['pptx', 'ppt']:
                # PowerPoint íŒŒì¼ì€ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜
                return f"PowerPoint ë¬¸ì„œ: {uploaded_file.name}\n(MarkItDown ë³€í™˜ ì‹¤íŒ¨, ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”)"
            else:
                return f"ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹: {uploaded_file.name}"
        except Exception as e:
            return f"Fallback í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"

    def analyze_document(self, content: str, filename: str, settings: Dict = None) -> Dict:
        """ë¬¸ì„œ ë¶„ì„ ì‹¤í–‰"""
        
        template = ANALYZE_DOCUMENT_PROMPT

        # LLM ë¶„ì„ ì‹œë„
        try:
            prompt = PromptTemplate.from_template(template)
            chain = prompt | llm | StrOutputParser()

            print("content: ", content[:50])

            response = chain.invoke({
                "content": content
            })

            # JSON ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ê¸°ì¡´ êµ¬ì¡°ì™€ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜
            import json
            # JSON ë¸”ë¡ì—ì„œ ì‹¤ì œ JSONë§Œ ì¶”ì¶œ
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response

            llm_result = json.loads(json_str)

            # ê¸°ì¡´ DocumentAnalyzer êµ¬ì¡°ì™€ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜
            enhanced_content = self._create_enhanced_content(content, llm_result)
            return {
                "original_content": content,
                "enhanced_content": enhanced_content,
                "quality_score": self._calculate_quality_score(content, llm_result),
                "original_length": len(content),
                "enhanced_length": len(enhanced_content),
                "issues_found": llm_result.get("improvements", []),
                "improvements": llm_result.get("improvements", []),
                "metadata": {
                    "analyzed_at": datetime.now(),
                    "filename": filename,
                    "analyzer_version": "LLM-1.0",
                    "llm_metadata": llm_result.get("metadata", {})
                },
                "llm_analysis": llm_result
            }
        except Exception as e:
            # LLM ë¶„ì„ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ DocumentAnalyzerë¡œ fallback
            print(f"LLM ë¶„ì„ ì‹¤íŒ¨ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë˜ëŠ” íŒŒì‹± ì‹¤íŒ¨), fallback ì‚¬ìš©: {e}")

    def _create_enhanced_content(self, original_content: str, llm_result: Dict) -> str:
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–¥ìƒëœ ì½˜í…ì¸  ìƒì„±"""
        metadata = llm_result.get("metadata", {})
        structure = llm_result.get("structure", [])
        usability = llm_result.get("usability", "")
        improvements = llm_result.get("improvements", [])

        enhanced = f"""# {metadata.get('topic', 'ë¬¸ì„œ ë¶„ì„ ê²°ê³¼')}

            ## ğŸ“‹ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            - **ë¬¸ì„œ ì¢…ë¥˜**: {metadata.get('type', 'ë¯¸ë¶„ë¥˜')}
            - **ì£¼ì œ**: {metadata.get('topic', 'ë¯¸í™•ì¸')}
            - **ì‘ì„±ì**: {metadata.get('author', 'ë¯¸í™•ì¸')}
            - **ì‘ì„±ì¼**: {metadata.get('date', 'ë¯¸í™•ì¸')}
            - **í”„ë¡œì íŠ¸/ë¶„ì•¼**: {metadata.get('project_area', 'ë¯¸í™•ì¸')}
            - **í‚¤ì›Œë“œ**: {', '.join(metadata.get('keywords', []))}

            ## ğŸ“‘ ì›ë³¸ ë‚´ìš©
            {original_content}

            ## ğŸ” ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
            """
        for section in structure:
            enhanced += f"\n### {section.get('section', 'ì„¹ì…˜')}\n{section.get('summary', 'ë‚´ìš© ìš”ì•½')}\n"

        enhanced += f"""
            ## ğŸ’¡ í™œìš© ê°€ëŠ¥ì„±
            {usability}

            ## âœ¨ ê°œì„  ì œì•ˆì‚¬í•­
            """
        for i, improvement in enumerate(improvements, 1):
            enhanced += f"{i}. {improvement}\n"

        enhanced += """
            ---
            *ë³¸ ë¬¸ì„œëŠ” AI ì§€ì‹ê´€ë¦¬ ì‹œìŠ¤í…œì— ì˜í•´ ë¶„ì„ ë° ë³´ì™„ë˜ì—ˆìŠµë‹ˆë‹¤.*
            """
        return enhanced.strip()

    def _calculate_quality_score(self, content: str, llm_result: Dict) -> int:
        """ì½˜í…ì¸ ì™€ LLM ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        base_score = 60

        # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
        length_score = min(len(content) // 100, 20)

        # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
        metadata = llm_result.get("metadata", {})
        metadata_score = 0
        for key in ["type", "topic", "author", "date", "project_area"]:
            if metadata.get(key) and metadata[key] != "ë¯¸í™•ì¸" and metadata[key] != "":
                metadata_score += 2

        # í‚¤ì›Œë“œ ê°œìˆ˜
        keywords_score = min(len(metadata.get("keywords", [])) * 1, 10)

        # êµ¬ì¡° ë¶„ì„ í’ˆì§ˆ
        structure_score = min(len(llm_result.get("structure", [])) * 3, 15)

        total_score = base_score + length_score + metadata_score + keywords_score + structure_score
        return min(total_score, 100)

    def _fallback_analysis(self, content: str, filename: str, error_msg: str) -> Dict:
        """LLM ë¶„ì„ ì‹¤íŒ¨ì‹œ fallback ë¶„ì„"""
        basic_result = self.document_analyzer.analyze_document(content, filename)

        # ì—ëŸ¬ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
        basic_result["metadata"]["error_info"] = {
            "llm_error": error_msg,
            "fallback_used": True,
            "error_time": datetime.now()
        }
        basic_result["metadata"]["analyzer_version"] = "Fallback-1.0"

        # ì—ëŸ¬ ê´€ë ¨ ê°œì„ ì‚¬í•­ ì¶”ê°€
        if "Connection error" in error_msg or "getaddrinfo failed" in error_msg:
            basic_result["issues_found"].insert(0, "âš ï¸ AI ë¶„ì„ ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨ - ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”")
            basic_result["improvements"].insert(0, "ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸ í›„ ì¬ì‹œë„")

        return basic_result

    def generate_enhanced_knowledge_document(self, analysis_result: Dict, filename: str) -> Dict:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ì™„ëœ ì§€ì‹ ë¬¸ì„œ ìƒì„±"""
        
        template = GENERATE_ENHANCED_KNOWLEDGE_DOCUMENT_PROMPT

        prompt = PromptTemplate.from_template(template)

        chain = prompt | llm | StrOutputParser()

        # LLMìœ¼ë¡œ ì§€ì‹ ë¬¸ì„œ ìƒì„± ì‹œë„
        try:
            response = chain.invoke({
                "original_content": analysis_result['original_content'],
                "improvement_points": analysis_result['improvements']
            })

            # ìƒì„±ëœ ë¬¸ì„œë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
            enhanced_document = {
                "enhanced_content": response,
                "quality_score": analysis_result.get('quality_score', 70),
                "generation_metadata": {
                    "generated_at": datetime.now(),
                    "filename": filename,
                    "method": "LLM Enhanced Generation",
                    "version": "1.0"
                }
            }

            return enhanced_document

        except Exception as e:
            # LLM ìƒì„± ì‹¤íŒ¨ì‹œ fallback
            print(f"LLM ì§€ì‹ ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")

            # ê¸°ë³¸ í…œí”Œë¦¿ìœ¼ë¡œ fallback
            fallback_content = f"""# {filename.split('.')[0]} - ì§€ì‹ ë¬¸ì„œ

                                    ## ğŸ“‹ ë©”íƒ€ë°ì´í„°
                                    - **ë¬¸ì„œ ì œëª©**: {filename}
                                    - **ì‘ì„±ì**: ë¯¸í™•ì¸
                                    - **ì‘ì„±ì¼**: {datetime.now().strftime('%Y-%m-%d')}
                                    - **ë²„ì „**: 1.0
                                    - **í”„ë¡œì íŠ¸/ì ìš© ë¶„ì•¼**: ë¯¸í™•ì¸
                                    - **ì£¼ìš” íƒœê·¸**: ë¶„ì„ ì¤‘

                                    ## ğŸ“– ë¬¸ì„œ ë³¸ë¬¸

                                    ### 1. ëª©ì  (Purpose)
                                    ì´ ë¬¸ì„œëŠ” ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œì„ í†µí•´ ë¶„ì„ëœ ë‚´ìš©ì„ ì •ë¦¬í•œ ë¬¸ì„œì…ë‹ˆë‹¤.

                                    ### 2. ì›ë³¸ ë‚´ìš©
                                    {analysis_result.get('original_content', 'ë‚´ìš© ì—†ìŒ')}

                                    ### 3. ë°œê²¬ëœ ê°œì„ ì‚¬í•­
                                    """
            for i, improvement in enumerate(analysis_result.get('improvements', []), 1):
                fallback_content += f"{i}. {improvement}\n"

            fallback_content += """
                                ### 4. ì ìš© ë° ì¬ì‚¬ìš© ë°©ì•ˆ
                                - í–¥í›„ ìœ ì‚¬í•œ í”„ë¡œì íŠ¸ì—ì„œ ì°¸ê³  ìë£Œë¡œ í™œìš© ê°€ëŠ¥
                                - ê°œì„ ì‚¬í•­ì„ ë°˜ì˜í•˜ì—¬ ë¬¸ì„œ í’ˆì§ˆ í–¥ìƒ í•„ìš”

                                ---
                                *ë³¸ ë¬¸ì„œëŠ” AI ì§€ì‹ê´€ë¦¬ ì‹œìŠ¤í…œì— ì˜í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
                                """

            enhanced_document = {
                "enhanced_content": fallback_content,
                "quality_score": analysis_result.get('quality_score', 70),
                "generation_metadata": {
                    "generated_at": datetime.now(),
                    "filename": filename,
                    "method": "Fallback Template Generation",
                    "version": "1.0",
                    "error": str(e)
                }
            }

            return enhanced_document

    def enhance_knote_to_standard_document(self, knote_json: Dict, additional_improvement_points: str = "") -> Dict:
        """K-Noteë¥¼ í‘œì¤€ ì§€ì‹ ë¬¸ì„œë¡œ ë³€í™˜"""
        
        # ì…ë ¥ ê²€ì¦
        if not isinstance(knote_json, dict):
            raise ValueError(f"knote_json must be a dictionary, got {type(knote_json)}")
        
        template = KNOTE_TO_STANDARD_DOC_PROMPT
        
        prompt = PromptTemplate.from_template(template)
        chain = prompt | llm | StrOutputParser()
        
        try:
            # K-Note JSONì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            import json
            
            def json_serializer(obj):
                """JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ serializer"""
                if isinstance(obj, datetime):
                    return obj.isoformat()
                raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
            
            knote_json_str = json.dumps(knote_json, ensure_ascii=False, indent=2, default=json_serializer)
            
            response = chain.invoke({
                "k_note_json": knote_json_str,
                "additional_improvement_points": additional_improvement_points or ""
            })
            
            # ìƒì„±ëœ í‘œì¤€ ë¬¸ì„œë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
            enhanced_document = {
                "enhanced_content": response,
                "quality_score": self._calculate_knote_quality_score(knote_json),
                "generation_metadata": {
                    "generated_at": datetime.now(),
                    "method": "K-Note to Standard Document Enhancement",
                    "version": "1.0",
                    "k_note_id": knote_json.get("k_note_id", "unknown")
                },
                "original_knote": knote_json
            }
            
            return enhanced_document
            
        except Exception as e:
            print(f"K-Note í‘œì¤€ ë¬¸ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            
            # Fallback: ê¸°ë³¸ í…œí”Œë¦¿ìœ¼ë¡œ ë³€í™˜
            fallback_content = self._create_fallback_standard_document(knote_json)
            
            enhanced_document = {
                "enhanced_content": fallback_content,
                "quality_score": self._calculate_knote_quality_score(knote_json),
                "generation_metadata": {
                    "generated_at": datetime.now(),
                    "method": "Fallback Standard Document Generation",
                    "version": "1.0",
                    "error": str(e),
                    "k_note_id": knote_json.get("k_note_id", "unknown")
                },
                "original_knote": knote_json
            }
            
            return enhanced_document

    def _calculate_knote_quality_score(self, knote_json: Dict) -> int:
        """K-Noteì˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        base_score = 70
        
        # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€
        required_fields = ["title", "proposal", "applicability", "evidence"]
        field_score = sum(10 for field in required_fields if knote_json.get(field))
        
        # Evidence í’ˆì§ˆ ì ìˆ˜
        evidence_score = 0
        evidence_list = knote_json.get("evidence", [])
        if evidence_list:
            # confidence ì ìˆ˜ í‰ê· 
            confidences = [e.get("confidence", 0) for e in evidence_list if isinstance(e, dict)]
            if confidences:
                avg_confidence = sum(confidences) / len(confidences)
                evidence_score = int(avg_confidence * 20)  # 0-1 ë²”ìœ„ë¥¼ 0-20ìœ¼ë¡œ ë³€í™˜
        
        # ì¶”ì²œ ì‹¤í—˜ ì¡´ì¬ ì—¬ë¶€
        experiment_score = 5 if knote_json.get("recommended_experiments") else 0
        
        total_score = base_score + field_score + evidence_score + experiment_score
        return min(total_score, 100)

    def _create_fallback_standard_document(self, knote_json: Dict) -> str:
        """K-Noteë¥¼ ê¸°ë³¸ í…œí”Œë¦¿ìœ¼ë¡œ ë³€í™˜ (fallback)"""
        title = knote_json.get("title", "ì§€ì‹ ë¬¸ì„œ")
        proposal = knote_json.get("proposal", "ë‚´ìš© ì—†ìŒ")
        owners = knote_json.get("owners", ["ë¯¸í™•ì¸"])
        version = knote_json.get("version", "1.0")
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []
        if knote_json.get("title"):
            keywords.extend(knote_json["title"].split())
        if knote_json.get("applicability", {}).get("when"):
            keywords.extend([item[:20] for item in knote_json["applicability"]["when"][:3]])
        
        fallback_content = f"""### ë©”íƒ€ë°ì´í„°
- ë¬¸ì„œ ì œëª©: {title}
- ì‘ì„±ì: {', '.join(owners) if isinstance(owners, list) else str(owners)}
- ì‘ì„±ì¼: ë¯¸í™•ì¸
- ë²„ì „: {version}
- í”„ë¡œì íŠ¸/ì ìš© ë¶„ì•¼: {knote_json.get('applicability', {}).get('when', ['ë¯¸í™•ì¸'])[0] if knote_json.get('applicability', {}).get('when') else 'ë¯¸í™•ì¸'}
- ì£¼ìš” íƒœê·¸(í‚¤ì›Œë“œ): {', '.join(keywords[:8]) if keywords else 'ë¯¸í™•ì¸'}

### ë¬¸ì„œ ë³¸ë¬¸

1. ëª©ì  (Purpose)
{proposal}

2. ë°°ê²½ ë° ë¬¸ì œ ì •ì˜ (Background / Problem Statement)
"""
        
        # ì ìš©ì„± ì •ë³´ ì¶”ê°€
        applicability = knote_json.get("applicability", {})
        if applicability.get("when"):
            fallback_content += f"ì ìš© ì¡°ê±´:\n"
            for condition in applicability["when"][:3]:
                fallback_content += f"- {condition}\n"
        
        if applicability.get("when_not"):
            fallback_content += f"\në¹„ì ìš© ì¡°ê±´:\n"
            for condition in applicability["when_not"][:3]:
                fallback_content += f"- {condition}\n"

        fallback_content += f"""
3. ì ‘ê·¼ ë°©ë²• ë° ì ˆì°¨ (Approach / Methodology)
{knote_json.get('proposal', 'ìƒì„¸ ë°©ë²•ë¡ ì€ ì›ë³¸ K-Noteë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.')}

4. ê²°ê³¼ ë° ì„±ê³¼ (Results / Outcomes)
"""
        
        # ë©”íŠ¸ë¦­ íš¨ê³¼ ì¶”ê°€
        metrics_effect = knote_json.get("metrics_effect", {})
        if metrics_effect:
            if isinstance(metrics_effect, dict):
                for key, value in metrics_effect.items():
                    fallback_content += f"- {key}: {value}\n"
            elif isinstance(metrics_effect, list):
                for item in metrics_effect:
                    fallback_content += f"- {str(item)}\n"
            else:
                fallback_content += f"- {str(metrics_effect)}\n"
        else:
            fallback_content += "ì„±ê³¼ ì§€í‘œëŠ” ì›ë³¸ K-Noteë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n"

        fallback_content += f"""
5. í•œê³„ ë° êµí›ˆ (Limitations / Lessons Learned)
"""
        
        # ìœ„í—˜ ë° í•œê³„ ì¶”ê°€
        risks_limits = knote_json.get("risks_limits", [])
        if risks_limits:
            # ì•ˆì „í•œ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
            if isinstance(risks_limits, list):
                for risk in risks_limits[:5]:
                    fallback_content += f"- {str(risk)}\n"
            else:
                fallback_content += f"- {str(risks_limits)}\n"
        else:
            fallback_content += "í•œê³„ì‚¬í•­ì€ ì›ë³¸ K-Noteë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n"

        fallback_content += f"""
6. ì ìš© ë° ì¬ì‚¬ìš© ë°©ì•ˆ (Application / Reusability)
"""
        
        # ì¶”ì²œ ì‹¤í—˜ ì¶”ê°€
        experiments = knote_json.get("recommended_experiments", [])
        if experiments:
            fallback_content += "ê¶Œì¥ ì‹¤í—˜/ì ìš© ì ˆì°¨:\n"
            # ì•ˆì „í•œ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
            if isinstance(experiments, list):
                for exp in experiments[:3]:
                    if isinstance(exp, dict):
                        fallback_content += f"- {exp.get('description', str(exp))}\n"
                    else:
                        fallback_content += f"- {str(exp)}\n"
            else:
                fallback_content += f"- {str(experiments)}\n"
        else:
            fallback_content += "ì¬ì‚¬ìš© ë°©ì•ˆì€ ì›ë³¸ K-Noteë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n"

        fallback_content += f"""
7. ì°¸ê³  ìë£Œ (References)
"""
        
        # Evidence ì¶”ê°€
        evidence_list = knote_json.get("evidence", [])
        if evidence_list:
            # ì•ˆì „í•œ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
            if isinstance(evidence_list, list):
                for i, evidence in enumerate(evidence_list[:5], 1):
                    if isinstance(evidence, dict):
                        doc_id = evidence.get("doc_id", "unknown")
                        chunk_id = evidence.get("chunk_id", "unknown")
                        quote = evidence.get("quote", "ë‚´ìš© ì—†ìŒ")
                        confidence = evidence.get("confidence", 0)
                        fallback_content += f"{i}. [{doc_id}#{chunk_id}] \"{quote[:100]}...\" (confidence: {confidence:.2f})\n"
                    else:
                        fallback_content += f"{i}. {str(evidence)}\n"
            else:
                fallback_content += f"1. {str(evidence_list)}\n"
        else:
            fallback_content += "ì¶”ê°€ ì¶œì²˜ í•„ìš”\n"

        fallback_content += f"""
### ë³´ì™„ì´ í•„ìš”í•œ ì  (Improvement Points)
- ì¹´í…Œê³ ë¦¬: ë¬¸ì„œí™” / ì´ìŠˆ: ìë™ ë³€í™˜ìœ¼ë¡œ ì¸í•œ ì •ë³´ ì†ì‹¤ / ì œì•ˆ: ì›ë³¸ K-Noteì™€ ëŒ€ì¡°í•˜ì—¬ ë‚´ìš© ë³´ì™„ / ê·¼ê±°: Fallback í…œí”Œë¦¿ ì‚¬ìš©

---
*ë³¸ ë¬¸ì„œëŠ” K-Noteì—ì„œ ìë™ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤. ì›ë³¸ K-Note ID: {knote_json.get('k_note_id', 'unknown')}*
"""
        
        return fallback_content

    def upload_enhanced_document_to_blob(self, enhanced_content: str, k_note_id: str, title: str) -> str:
        """êµ¬ì²´í™”ëœ ë¬¸ì„œë¥¼ Azure Blob Storage enhanced ì»¨í…Œì´ë„ˆì— ì—…ë¡œë“œ"""
        try:
            import io
            from datetime import datetime
            
            # íŒŒì¼ëª… ìƒì„± (K-Note IDì™€ ì œëª© ê¸°ë°˜)
            safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{k_note_id}_{safe_title}_{timestamp}_enhanced.md"
            
            # ë¬¸ì„œ ë‚´ìš©ì„ íŒŒì¼ í˜•íƒœë¡œ ë³€í™˜
            enhanced_file = io.BytesIO(enhanced_content.encode('utf-8'))
            enhanced_file.name = filename
            
            # FileProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ ì—…ë¡œë“œ
            enhanced_url = self.file_processor.upload_file(enhanced_file, "enhanced")
            
            return enhanced_url
            
        except Exception as e:
            print(f"Enhanced ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def save_to_vector_db(self, analysis_result: Dict) -> Dict:
        """VectorDBì— ì €ì¥"""
        try:
            vector_entry = {
                "content": analysis_result['enhanced_content'],
                "filename": st.session_state.current_file_info['name'],
                "metadata": analysis_result.get('metadata', {}),
                "quality_score": analysis_result['quality_score'],
                "timestamp": datetime.now(),
                "original_length": analysis_result['original_length'],
                "analysis_type": st.session_state.get('analysis_settings', {}).get('depth', 'ê¸°ë³¸')
            }

            if 'vector_db' not in st.session_state:
                st.session_state.vector_db = []

            st.session_state.vector_db.append(vector_entry)

            return {
                "success": True,
                "message": "VectorDB ì €ì¥ ì™„ë£Œ",
                "count": len(st.session_state.vector_db)
            }

        except Exception as e:
            return {
                "success": False,
                "message": f"ì €ì¥ ì‹¤íŒ¨: {str(e)}"
            }

    def get_knowledge_stats(self) -> Dict:
        """ì§€ì‹ ê´€ë¦¬ í†µê³„"""
        vector_db = st.session_state.get('vector_db', [])
        board_posts = st.session_state.get('board_posts', [])

        # í‰ê·  í’ˆì§ˆ ì ìˆ˜
        all_scores = []
        for doc in vector_db:
            if 'quality_score' in doc:
                all_scores.append(doc['quality_score'])
        for post in board_posts:
            if 'quality_score' in post:
                all_scores.append(post['quality_score'])

        avg_quality = sum(all_scores) / len(all_scores) if all_scores else 0

        # ì˜¤ëŠ˜ ë“±ë¡ëœ ë¬¸ì„œ ìˆ˜
        today = datetime.now().date()
        today_count = 0

        for doc in vector_db:
            if doc.get('timestamp') and doc['timestamp'].date() == today:
                today_count += 1

        for post in board_posts:
            if post.get('timestamp') and post['timestamp'].date() == today:
                today_count += 1

        return {
            'vector_count': len(vector_db),
            'board_count': len(board_posts),
            'avg_quality': avg_quality,
            'today_count': today_count,
            'total_documents': len(vector_db) + len(board_posts)
        }

    def get_file_info(self, uploaded_file) -> Dict:
        """íŒŒì¼ ì •ë³´ ë°˜í™˜"""
        return self.file_processor.get_file_info(uploaded_file)

    def search_knowledge(self, query: str) -> Dict:
        """ì§€ì‹ ê²€ìƒ‰"""
        vector_db = st.session_state.get('vector_db', [])
        board_posts = st.session_state.get('board_posts', [])

        results = {
            'vector_results': [],
            'board_results': [],
            'total_count': 0
        }

        query_lower = query.lower()

        # VectorDB ê²€ìƒ‰
        for doc in vector_db:
            if query_lower in doc.get('content', '').lower() or query_lower in doc.get('filename', '').lower():
                results['vector_results'].append(doc)

        # ê²Œì‹œíŒ ê²€ìƒ‰
        for post in board_posts:
            if query_lower in post.get('content', '').lower() or query_lower in post.get('title', '').lower():
                results['board_results'].append(post)

        results['total_count'] = len(results['vector_results']) + len(results['board_results'])
        return results


class RAGService:
    """RAGë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ë¶„í•  ë° ë²¡í„° DB ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, chroma_persist_directory: str = "./data/chroma_db",
                 sqlite_db_path: str = "./data/board.db"):
        """
        RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”

        Args:
            chroma_persist_directory: ChromaDB ì €ì¥ ê²½ë¡œ
            sqlite_db_path: SQLite ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        """
        # Azure OpenAI Embeddings ì´ˆê¸°í™”
        self.embeddings = AzureOpenAIEmbeddings(
            azure_endpoint=endpoint,
            api_key=api_key,
            api_version=embedding_api_version,
            azure_deployment=embedding_deployment
        )

        # ChromaDB ì´ˆê¸°í™”
        self.chroma_persist_directory = chroma_persist_directory
        os.makedirs(chroma_persist_directory, exist_ok=True)

        # SQLite DB ì´ˆê¸°í™”
        self.sqlite_db_path = sqlite_db_path
        os.makedirs(os.path.dirname(sqlite_db_path), exist_ok=True)
        self._initialize_board_db()

    def _initialize_board_db(self):
        """ê²Œì‹œíŒ DB í…Œì´ë¸” ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.sqlite_db_path)
        cursor = conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS board_posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                enhanced_doc_url TEXT,
                original_doc_url TEXT,
                author TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                views INTEGER DEFAULT 0,
                quality_score INTEGER DEFAULT 0,
                metadata TEXT
            )
        """)

        conn.commit()
        conn.close()

    def split_text(self, text: str, split_type: str = "semantic",
                   chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            split_type: ë¶„í•  ë°©ì‹ ("character", "recursive", "semantic", "token")
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°

        Returns:
            ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if split_type == "character":
                splitter = CharacterTextSplitter(
                    separator="\n\n",
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
            elif split_type == "recursive":
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    separators=["\n\n", "\n", " ", ""]
                )
            elif split_type == "semantic":
                splitter = SemanticChunker(
                    embeddings=self.embeddings,
                    breakpoint_threshold_type="gradient"
                )
            elif split_type == "token":
                splitter = TokenTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„í•  ë°©ì‹: {split_type}")

            chunks = splitter.split_text(text)
            return chunks

        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # Fallback: recursive splitter ì‚¬ìš©
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            return splitter.split_text(text)

    def embed_and_store(self, text: str, metadata: Dict = None,
                       split_type: str = "semantic",
                       chunk_size: int = 1000,
                       chunk_overlap: int = 200,
                       collection_name: str = "knowledge_base") -> Dict:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ë²¡í„° DBì— ì„ë² ë”©í•˜ì—¬ ì €ì¥

        Args:
            text: ì €ì¥í•  í…ìŠ¤íŠ¸
            metadata: ë©”íƒ€ë°ì´í„°
            split_type: ë¶„í•  ë°©ì‹
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„

        Returns:
            ì €ì¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self.split_text(text, split_type, chunk_size, chunk_overlap)

            if not chunks:
                return {
                    "success": False,
                    "message": "í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨",
                    "chunk_count": 0
                }

            # ë©”íƒ€ë°ì´í„° ì„¤ì •
            if metadata is None:
                metadata = {}

            # ê° ì²­í¬ì— ê³ ìœ  ID ë¶€ì—¬
            doc_id = metadata.get('doc_id', str(uuid.uuid4()))
            metadata['doc_id'] = doc_id

            # ChromaDB í˜¸í™˜ ë©”íƒ€ë°ì´í„°ë¡œ ì •ì œ (None ê°’ ì œê±°, ë³µì¡í•œ íƒ€ì… ì œê±°)
            clean_metadata = self._clean_metadata_for_chroma(metadata)

            # ChromaDBì— ì €ì¥
            vectorstore = Chroma.from_texts(
                texts=chunks,
                embedding=self.embeddings,
                metadatas=[{**clean_metadata, 'chunk_id': i} for i in range(len(chunks))],
                collection_name=collection_name,
                persist_directory=self.chroma_persist_directory
            )

            return {
                "success": True,
                "message": "VectorDB ì €ì¥ ì™„ë£Œ",
                "chunk_count": len(chunks),
                "doc_id": doc_id,
                "collection_name": collection_name
            }

        except Exception as e:
            print(f"VectorDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "success": False,
                "message": f"ì €ì¥ ì‹¤íŒ¨: {str(e)}",
                "chunk_count": 0
            }

    def _clean_metadata_for_chroma(self, metadata: Dict) -> Dict:
        """
        ChromaDB í˜¸í™˜ ë©”íƒ€ë°ì´í„°ë¡œ ì •ì œ
        - None ê°’ ì œê±°
        - str, int, float, boolë§Œ í—ˆìš©
        - ì¤‘ì²© ë”•ì…”ë„ˆë¦¬ë‚˜ ë¦¬ìŠ¤íŠ¸ëŠ” JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        """
        clean = {}
        for key, value in metadata.items():
            if value is None:
                continue  # None ê°’ì€ ì œì™¸
            elif isinstance(value, (str, int, float, bool)):
                clean[key] = value
            elif isinstance(value, (dict, list)):
                # ë³µì¡í•œ íƒ€ì…ì€ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
                try:
                    clean[key] = json.dumps(value, ensure_ascii=False)
                except:
                    clean[key] = str(value)
            else:
                # ê¸°íƒ€ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                clean[key] = str(value)
        return clean

    def search_similar(self, query: str, k: int = 5,
                      collection_name: str = "knowledge_base") -> List[Dict]:
        """
        ìœ ì‚¬ë„ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜
            collection_name: ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ ì´ë¦„

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            vectorstore = Chroma(
                collection_name=collection_name,
                embedding_function=self.embeddings,
                persist_directory=self.chroma_persist_directory
            )

            results = vectorstore.similarity_search_with_score(query, k=k)

            formatted_results = []
            for doc, score in results:
                formatted_results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "similarity_score": float(score)
                })

            return formatted_results

        except Exception as e:
            print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def save_to_board_db(self, title: str, content: str,
                        enhanced_doc_url: str = None,
                        original_doc_url: str = None,
                        author: str = "AI Knowledge System",
                        quality_score: int = 0,
                        metadata: Dict = None) -> Dict:
        """
        ê²Œì‹œíŒ DBì— ë¬¸ì„œ ì €ì¥

        Args:
            title: ë¬¸ì„œ ì œëª©
            content: ë¬¸ì„œ ë‚´ìš© (ë³´ì™„ëœ ë‚´ìš©)
            enhanced_doc_url: ë³´ì™„ëœ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬
            original_doc_url: ì›ë³¸ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬
            author: ì‘ì„±ì
            quality_score: í’ˆì§ˆ ì ìˆ˜
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥)

        Returns:
            ì €ì¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            conn = sqlite3.connect(self.sqlite_db_path)
            cursor = conn.cursor()

            post_id = str(uuid.uuid4())
            import json

            # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if metadata:
                metadata_copy = {}
                for key, value in metadata.items():
                    if isinstance(value, dict):
                        # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
                        metadata_copy[key] = {}
                        for k, v in value.items():
                            if isinstance(v, datetime):
                                metadata_copy[key][k] = v.isoformat()
                            else:
                                metadata_copy[key][k] = v
                    elif isinstance(value, datetime):
                        metadata_copy[key] = value.isoformat()
                    else:
                        metadata_copy[key] = value
                metadata_json = json.dumps(metadata_copy, ensure_ascii=False)
            else:
                metadata_json = None

            cursor.execute("""
                INSERT INTO board_posts
                (id, title, content, enhanced_doc_url, original_doc_url,
                 author, quality_score, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (post_id, title, content, enhanced_doc_url, original_doc_url,
                  author, quality_score, metadata_json))

            conn.commit()
            conn.close()

            return {
                "success": True,
                "message": "ê²Œì‹œíŒ ì €ì¥ ì™„ë£Œ",
                "post_id": post_id
            }

        except Exception as e:
            print(f"ê²Œì‹œíŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "success": False,
                "message": f"ì €ì¥ ì‹¤íŒ¨: {str(e)}"
            }

    def get_board_posts(self, limit: int = 100, offset: int = 0) -> List[Dict]:
        """
        ê²Œì‹œíŒ ê²Œì‹œê¸€ ì¡°íšŒ

        Args:
            limit: ì¡°íšŒí•  ê²Œì‹œê¸€ ìˆ˜
            offset: ì‹œì‘ ìœ„ì¹˜

        Returns:
            ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
        """
        try:
            conn = sqlite3.connect(self.sqlite_db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            cursor.execute("""
                SELECT * FROM board_posts
                ORDER BY created_at DESC
                LIMIT ? OFFSET ?
            """, (limit, offset))

            rows = cursor.fetchall()
            conn.close()

            import json
            posts = []
            for row in rows:
                post = dict(row)
                if post.get('metadata'):
                    post['metadata'] = json.loads(post['metadata'])
                posts.append(post)

            return posts

        except Exception as e:
            print(f"ê²Œì‹œê¸€ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def get_board_post_by_id(self, post_id: str) -> Optional[Dict]:
        """
        íŠ¹ì • ê²Œì‹œê¸€ ì¡°íšŒ ë° ì¡°íšŒìˆ˜ ì¦ê°€

        Args:
            post_id: ê²Œì‹œê¸€ ID

        Returns:
            ê²Œì‹œê¸€ ì •ë³´
        """
        try:
            conn = sqlite3.connect(self.sqlite_db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # ì¡°íšŒìˆ˜ ì¦ê°€
            cursor.execute("""
                UPDATE board_posts
                SET views = views + 1
                WHERE id = ?
            """, (post_id,))

            # ê²Œì‹œê¸€ ì¡°íšŒ
            cursor.execute("""
                SELECT * FROM board_posts WHERE id = ?
            """, (post_id,))

            row = cursor.fetchone()
            conn.commit()
            conn.close()

            if row:
                import json
                post = dict(row)
                if post.get('metadata'):
                    post['metadata'] = json.loads(post['metadata'])
                return post

            return None

        except Exception as e:
            print(f"ê²Œì‹œê¸€ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def delete_board_post(self, post_id: str) -> Dict:
        """
        ê²Œì‹œê¸€ ì‚­ì œ

        Args:
            post_id: ê²Œì‹œê¸€ ID

        Returns:
            ì‚­ì œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            conn = sqlite3.connect(self.sqlite_db_path)
            cursor = conn.cursor()

            cursor.execute("DELETE FROM board_posts WHERE id = ?", (post_id,))

            conn.commit()
            deleted = cursor.rowcount > 0
            conn.close()

            if deleted:
                return {
                    "success": True,
                    "message": "ê²Œì‹œê¸€ ì‚­ì œ ì™„ë£Œ"
                }
            else:
                return {
                    "success": False,
                    "message": "ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                }

        except Exception as e:
            print(f"ê²Œì‹œê¸€ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "success": False,
                "message": f"ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
            }