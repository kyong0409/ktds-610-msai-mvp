"""
ì§€ì‹ ê´€ë¦¬ ì„œë¹„ìŠ¤ ë¡œì§
"""
import streamlit as st
import time
from datetime import datetime
from typing import Dict, Any, Tuple, Optional
from utils.file_processor import FileProcessor
from services.document_analyzer import DocumentAnalyzer
from markitdown import MarkItDown
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from dotenv import load_dotenv
import os
from azure.core.credentials import AzureKeyCredential
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.text_splitter import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
    TokenTextSplitter
)
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import Chroma
import sqlite3
import uuid
from typing import List

load_dotenv()

AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
AZURE_DOCUMENT_INTELLIGENCE_KEY = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_KEY")

key = AzureKeyCredential(AZURE_DOCUMENT_INTELLIGENCE_KEY)

# Azure OpenAI ì„¤ì •
endpoint = os.environ["AZURE_ENDPOINT"]
api_key = os.environ["AZURE_AI_FOUNDRY_KEY"]
api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-12-01-preview")
embedding_api_version = os.getenv("AZURE_OPENAI_EMBEDDING_API_VERSION", "2024-12-01-preview")
embedding_deployment = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-3-large")

print("endpoint: ", endpoint)
print("api_key: ", api_key)
print("api_version: ", api_version)

# ë°°í¬ ì´ë¦„: Foundryì—ì„œ ë§Œë“  ë°°í¬ëª…ê³¼ ë™ì¼í•´ì•¼ í•¨
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4.1-mini")
print("DEPLOYMENT_NAME: ", DEPLOYMENT_NAME)
llm = AzureChatOpenAI(
		azure_endpoint=endpoint,
		api_key=api_key,
		api_version=api_version,
		azure_deployment=DEPLOYMENT_NAME,
		temperature=0.4,
		max_retries=3,
		timeout=30,
)

class KnowledgeService:
    """ì§€ì‹ ê´€ë¦¬ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        self.file_processor = FileProcessor()
        self.document_analyzer = DocumentAnalyzer()
        self.markitdown = MarkItDown(docintel_endpoint=AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT, docintel_credential=key)

    def process_uploaded_file(self, uploaded_file) -> Tuple[Optional[Dict], Optional[str]]:
        """ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬"""
        # íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
        is_valid, error_message = self.file_processor.validate_file(uploaded_file)

        if not is_valid:
            st.error(error_message)
            return None, None

        # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        file_info = self.file_processor.get_file_info(uploaded_file)

        # íŒŒì¼ ë‚´ìš© ì¶”ì¶œ
        file_content = self.file_processor.extract_text(uploaded_file)

        return file_info, file_content

    def convert_file_to_text(self, uploaded_file) -> Tuple[bool, str]:
        """MarkItDownì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            import tempfile
            import os

            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_file_path = tmp_file.name

            # MarkItDownìœ¼ë¡œ ë³€í™˜
            result = self.markitdown.convert(tmp_file_path)

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)

            # ì—…ë¡œë“œ íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            uploaded_file.seek(0)

            return True, result.text_content

        except Exception as e:
            # ì—…ë¡œë“œ íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            uploaded_file.seek(0)

            # Fallback: ê¸°ì¡´ íŒŒì¼ ì²˜ë¦¬ ë°©ì‹ ì‚¬ìš©
            fallback_content = self._fallback_text_extraction(uploaded_file)
            if fallback_content:
                return True, fallback_content

            return False, f"íŒŒì¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def _fallback_text_extraction(self, uploaded_file) -> str:
        """Fallback í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹)"""
        try:
            # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            uploaded_file.seek(0)

            file_type = uploaded_file.type

            if file_type == "text/plain":
                return str(uploaded_file.read(), "utf-8")
            elif file_type == "application/pdf":
                try:
                    import PyPDF2
                    import io

                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.read()))
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                    return text
                except:
                    return f"[PDF íŒŒì¼: {uploaded_file.name}] - PDF ë³€í™˜ì„ ìœ„í•´ 'pip install markitdown[pdf]'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            elif file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                try:
                    from docx import Document
                    import io

                    doc = Document(io.BytesIO(uploaded_file.read()))
                    text = ""
                    for paragraph in doc.paragraphs:
                        text += paragraph.text + "\n"
                    return text
                except:
                    return f"[Word íŒŒì¼: {uploaded_file.name}] - íŒŒì¼ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                return f"[{uploaded_file.name}] - ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."

        except Exception as e:
            return f"[íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜] {str(e)}"
        finally:
            # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            uploaded_file.seek(0)

    def get_file_preview(self, uploaded_file) -> str:
        """íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸ ë°˜í™˜"""
        success, content = self.convert_file_to_text(uploaded_file)

        if success:
            # ë¯¸ë¦¬ë³´ê¸°ëŠ” ì²˜ìŒ 1000ìë§Œ í‘œì‹œ
            preview_text = content[:1000]
            if len(content) > 1000:
                preview_text += "\n\n... (ê³„ì†)"
            return preview_text
        else:
            return content  # ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜

    def analyze_document(self, content: str, filename: str, settings: Dict = None) -> Dict:
        """ë¬¸ì„œ ë¶„ì„ ì‹¤í–‰"""
        
        template = """
        [ì—­í• ]
        ë‹¹ì‹ ì€ IT íšŒì‚¬ì˜ ì§€ì‹ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì…ë ¥ëœ í…ìŠ¤íŠ¸ ë¬¸ì„œ(ì›ë³¸ ì§€ì‹ ë¬¸ì„œ)ë¥¼ ë¶„ì„í•˜ì—¬, ì§€ì‹ìì‚°í™”ì— í•„ìš”í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ê°œì„ ì´ í•„ìš”í•œ ë³´ì™„ì ì„ ë„ì¶œí•˜ì„¸ìš”.

        [ì…ë ¥]
        {content}

        [ì§€ì‹œì‚¬í•­]
        ë‹¤ìŒ í•­ëª©ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        ## 1. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        - ë¬¸ì„œ ì¢…ë¥˜: {{PoC ë³´ê³ ì„œ | Lessons Learned | ê¸°ìˆ ìë£Œ | í”„ë¡œì íŠ¸ ì‚°ì¶œë¬¼ | ê¸°íƒ€}}
        - ì£¼ì œ(Topic): í•œ ì¤„ ìš”ì•½
        - ì‘ì„±ì¼/ì‘ì„±ì: ì›ë¬¸ì—ì„œ ë°œê²¬ë˜ë©´ ì¶”ì¶œ, ì—†ìœ¼ë©´ "ë¯¸í™•ì¸"
        - í”„ë¡œì íŠ¸/ì ìš© ë¶„ì•¼: ë¬¸ë§¥ì—ì„œ ìœ ì¶”
        - ì£¼ìš” í‚¤ì›Œë“œ(íƒœê·¸): í•µì‹¬ ê¸°ìˆ , ë„ë©”ì¸, ê´€ë ¨ ìš©ì–´ë¥¼ 5~10ê°œ

        ## 2. ë¬¸ì„œ êµ¬ì¡°/ëª©ì°¨ ë¶„ì„
        - ë¬¸ì„œ ë‚´ ì¡´ì¬í•˜ëŠ” ì£¼ìš” ì„¹ì…˜/í•­ëª© ëª©ë¡í™”
        - ê° ì„¹ì…˜ì´ ë‹¤ë£¨ëŠ” ë‚´ìš© ìš”ì•½

        ## 3. í™œìš© ê°€ëŠ¥ì„± ë¶„ì„
        - ì´ ë¬¸ì„œê°€ ì§€ì‹ìì‚°ìœ¼ë¡œì„œ ì–´ë–¤ ê°€ì¹˜ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆëŠ”ì§€
        - ì¬ì‚¬ìš©/ì°¸ì¡° ê°€ëŠ¥í•œ ë¶€ë¶„

        ## 4. ë³´ì™„ì´ í•„ìš”í•œ ì 
        - ë¹ ì§„ í•­ëª© (ì˜ˆ: ëª©ì , ê²°ê³¼, êµí›ˆ, ì ìš© ë°©ì•ˆ ë“±)
        - ë¶ˆëª…í™•í•˜ê±°ë‚˜ ì •ë¦¬ë˜ì§€ ì•Šì€ ë¶€ë¶„
        - ê²€ìƒ‰/ì¬ì‚¬ìš© ê´€ì ì—ì„œ ê°œì„ í•´ì•¼ í•  ì 

        [ì¶œë ¥ í˜•ì‹]
        ì•„ë˜ JSON êµ¬ì¡°ë¡œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

        ```json
        {{
            "metadata": {{
                "type": "",
                "topic": "",
                "author": "",
                "date": "",
                "project_area": "",
                "keywords": []
            }},
            "structure": [
                {{
                "section": "",
                "summary": ""
                }}
            ],
            "usability": "ì´ ë¬¸ì„œê°€ ì§€ì‹ìì‚°ìœ¼ë¡œì„œ ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆëŠ”ì§€ ì„¤ëª…",
            "improvements": [
                "ë³´ì™„ì 1",
                "ë³´ì™„ì 2",
                "ë³´ì™„ì 3"
            ]
        }}
        """

        # LLM ë¶„ì„ ì‹œë„
        try:
            prompt = PromptTemplate.from_template(template)
            chain = prompt | llm | StrOutputParser()

            print("content: ", content[:50])

            response = chain.invoke({
                "content": content
            })

            # JSON ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ê¸°ì¡´ êµ¬ì¡°ì™€ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜
            import json
            # JSON ë¸”ë¡ì—ì„œ ì‹¤ì œ JSONë§Œ ì¶”ì¶œ
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response

            llm_result = json.loads(json_str)

            # ê¸°ì¡´ DocumentAnalyzer êµ¬ì¡°ì™€ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜
            enhanced_content = self._create_enhanced_content(content, llm_result)
            return {
                "original_content": content,
                "enhanced_content": enhanced_content,
                "quality_score": self._calculate_quality_score(content, llm_result),
                "original_length": len(content),
                "enhanced_length": len(enhanced_content),
                "issues_found": llm_result.get("improvements", []),
                "improvements": llm_result.get("improvements", []),
                "metadata": {
                    "analyzed_at": datetime.now(),
                    "filename": filename,
                    "analyzer_version": "LLM-1.0",
                    "llm_metadata": llm_result.get("metadata", {})
                },
                "llm_analysis": llm_result
            }
        except Exception as e:
            # LLM ë¶„ì„ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ DocumentAnalyzerë¡œ fallback
            print(f"LLM ë¶„ì„ ì‹¤íŒ¨ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë˜ëŠ” íŒŒì‹± ì‹¤íŒ¨), fallback ì‚¬ìš©: {e}")
            return self._fallback_analysis(content, filename, str(e))

    def _create_enhanced_content(self, original_content: str, llm_result: Dict) -> str:
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–¥ìƒëœ ì½˜í…ì¸  ìƒì„±"""
        metadata = llm_result.get("metadata", {})
        structure = llm_result.get("structure", [])
        usability = llm_result.get("usability", "")
        improvements = llm_result.get("improvements", [])

        enhanced = f"""# {metadata.get('topic', 'ë¬¸ì„œ ë¶„ì„ ê²°ê³¼')}

## ğŸ“‹ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
- **ë¬¸ì„œ ì¢…ë¥˜**: {metadata.get('type', 'ë¯¸ë¶„ë¥˜')}
- **ì£¼ì œ**: {metadata.get('topic', 'ë¯¸í™•ì¸')}
- **ì‘ì„±ì**: {metadata.get('author', 'ë¯¸í™•ì¸')}
- **ì‘ì„±ì¼**: {metadata.get('date', 'ë¯¸í™•ì¸')}
- **í”„ë¡œì íŠ¸/ë¶„ì•¼**: {metadata.get('project_area', 'ë¯¸í™•ì¸')}
- **í‚¤ì›Œë“œ**: {', '.join(metadata.get('keywords', []))}

## ğŸ“‘ ì›ë³¸ ë‚´ìš©
{original_content}

## ğŸ” ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
"""
        for section in structure:
            enhanced += f"\n### {section.get('section', 'ì„¹ì…˜')}\n{section.get('summary', 'ë‚´ìš© ìš”ì•½')}\n"

        enhanced += f"""
## ğŸ’¡ í™œìš© ê°€ëŠ¥ì„±
{usability}

## âœ¨ ê°œì„  ì œì•ˆì‚¬í•­
"""
        for i, improvement in enumerate(improvements, 1):
            enhanced += f"{i}. {improvement}\n"

        enhanced += """
---
*ë³¸ ë¬¸ì„œëŠ” AI ì§€ì‹ê´€ë¦¬ ì‹œìŠ¤í…œì— ì˜í•´ ë¶„ì„ ë° ë³´ì™„ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
        return enhanced.strip()

    def _calculate_quality_score(self, content: str, llm_result: Dict) -> int:
        """ì½˜í…ì¸ ì™€ LLM ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        base_score = 60

        # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
        length_score = min(len(content) // 100, 20)

        # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
        metadata = llm_result.get("metadata", {})
        metadata_score = 0
        for key in ["type", "topic", "author", "date", "project_area"]:
            if metadata.get(key) and metadata[key] != "ë¯¸í™•ì¸" and metadata[key] != "":
                metadata_score += 2

        # í‚¤ì›Œë“œ ê°œìˆ˜
        keywords_score = min(len(metadata.get("keywords", [])) * 1, 10)

        # êµ¬ì¡° ë¶„ì„ í’ˆì§ˆ
        structure_score = min(len(llm_result.get("structure", [])) * 3, 15)

        total_score = base_score + length_score + metadata_score + keywords_score + structure_score
        return min(total_score, 100)

    def _fallback_analysis(self, content: str, filename: str, error_msg: str) -> Dict:
        """LLM ë¶„ì„ ì‹¤íŒ¨ì‹œ fallback ë¶„ì„"""
        basic_result = self.document_analyzer.analyze_document(content, filename)

        # ì—ëŸ¬ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
        basic_result["metadata"]["error_info"] = {
            "llm_error": error_msg,
            "fallback_used": True,
            "error_time": datetime.now()
        }
        basic_result["metadata"]["analyzer_version"] = "Fallback-1.0"

        # ì—ëŸ¬ ê´€ë ¨ ê°œì„ ì‚¬í•­ ì¶”ê°€
        if "Connection error" in error_msg or "getaddrinfo failed" in error_msg:
            basic_result["issues_found"].insert(0, "âš ï¸ AI ë¶„ì„ ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨ - ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”")
            basic_result["improvements"].insert(0, "ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸ í›„ ì¬ì‹œë„")

        return basic_result

    def generate_enhanced_knowledge_document(self, analysis_result: Dict, filename: str) -> Dict:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ì™„ëœ ì§€ì‹ ë¬¸ì„œ ìƒì„±"""
        
        template = """
        [ì—­í• ]  
        ë‹¹ì‹ ì€ IT íšŒì‚¬ì˜ ì§€ì‹ ê´€ë¦¬ ì „ë¬¸ê°€ì´ì ê¸°ìˆ  ë¬¸ì„œ í¸ì§‘ìì…ë‹ˆë‹¤.  
        ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì…ë ¥ëœ ë¬¸ì„œì™€ ë³´ì™„ì‚¬í•­ì„ ì¢…í•©í•˜ì—¬, ì‚¬ë‚´ì—ì„œ í™œìš© ê°€ëŠ¥í•œ "í‘œì¤€ ì§€ì‹ ë¬¸ì„œ" í˜•íƒœë¡œ ì¬êµ¬ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.  

        [ì…ë ¥]  
        1. ì›ë³¸ ë¬¸ì„œ (Original Document)  
        {original_content}
        2. ë³´ì™„ì‚¬í•­ (Improvement Points)  
        {improvement_points}

        [ì§€ì‹œì‚¬í•­]  
        1. ì›ë³¸ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ **í•µì‹¬ ë‚´ìš©**ì„ ìœ ì§€í•©ë‹ˆë‹¤.  
        2. ë³´ì™„ì‚¬í•­ì„ ë°˜ì˜í•˜ì—¬ ë‚´ìš©ì˜ ê³µë°±ì„ ì±„ìš°ê±°ë‚˜ í‘œí˜„ì„ ê°œì„ í•©ë‹ˆë‹¤.  
        3. ìµœì¢… ì‚°ì¶œë¬¼ì€ ë‹¤ìŒ í‘œì¤€ í˜•ì‹ì„ ë”°ë¦…ë‹ˆë‹¤:  

        ### ë©”íƒ€ë°ì´í„°
        - ë¬¸ì„œ ì œëª©: # ë¬¸ì„œì˜ ì£¼ì œ/í•µì‹¬ ë‚´ìš©ì´ ì˜ ë‚˜íƒ€ë‚˜ë„ë¡ ì‘ì„±í•©ë‹ˆë‹¤.  
        - ì‘ì„±ì: ì›ë¬¸ì—ì„œ ë°œê²¬ë˜ë©´ ì¶”ì¶œ, ì—†ìœ¼ë©´ "ë¯¸í™•ì¸"
        - ì‘ì„±ì¼:  
        - ë²„ì „:  
        - í”„ë¡œì íŠ¸/ì ìš© ë¶„ì•¼: ë¬¸ë§¥ì—ì„œ ìœ ì¶”
        - ì£¼ìš” íƒœê·¸(í‚¤ì›Œë“œ):  

        ### ë¬¸ì„œ ë³¸ë¬¸
        1. ëª©ì  (Purpose)  
        2. ë°°ê²½ ë° ë¬¸ì œ ì •ì˜ (Background / Problem Statement)  
        3. ì ‘ê·¼ ë°©ë²• ë° ì ˆì°¨ (Approach / Methodology)  
        4. ê²°ê³¼ ë° ì„±ê³¼ (Results / Outcomes)  
        5. í•œê³„ ë° êµí›ˆ (Limitations / Lessons Learned)  
        6. ì ìš© ë° ì¬ì‚¬ìš© ë°©ì•ˆ (Application / Reusability)  
        7. ì°¸ê³  ìë£Œ (References)  

        4. ë‚´ìš©ì´ ë¶€ì¡±í•œ ê²½ìš°, ë³´ì™„ì‚¬í•­ì„ í™œìš©í•˜ì—¬ í•©ë¦¬ì ìœ¼ë¡œ ë³´ê°•í•©ë‹ˆë‹¤. ë‹¨, ê·¼ê±° ì—†ì´ ìƒˆë¡œìš´ ì‚¬ì‹¤ì„ ì°½ì‘í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.  
        5. í‘œí˜„ì€ **ëª…í™•í•˜ê³  ê°„ê²°í•˜ë©°, ì‚¬ë‚´ ê²€ìƒ‰/í•™ìŠµì— ì í•©í•œ ìš©ì–´**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  

        [ì¶œë ¥]  
        ìœ„ í˜•ì‹ì— ë§ì¶˜ **ì™„ì„±ëœ ì§€ì‹ ë¬¸ì„œ**ë¥¼ ì œê³µí•©ë‹ˆë‹¤.  
        """

        prompt = PromptTemplate.from_template(template)

        chain = prompt | llm | StrOutputParser()

        # LLMìœ¼ë¡œ ì§€ì‹ ë¬¸ì„œ ìƒì„± ì‹œë„
        try:
            response = chain.invoke({
                "original_content": analysis_result['original_content'],
                "improvement_points": analysis_result['improvements']
            })

            # ìƒì„±ëœ ë¬¸ì„œë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
            enhanced_document = {
                "enhanced_content": response,
                "quality_score": analysis_result.get('quality_score', 70),
                "generation_metadata": {
                    "generated_at": datetime.now(),
                    "filename": filename,
                    "method": "LLM Enhanced Generation",
                    "version": "1.0"
                }
            }

            return enhanced_document

        except Exception as e:
            # LLM ìƒì„± ì‹¤íŒ¨ì‹œ fallback
            print(f"LLM ì§€ì‹ ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")

            # ê¸°ë³¸ í…œí”Œë¦¿ìœ¼ë¡œ fallback
            fallback_content = f"""# {filename.split('.')[0]} - ì§€ì‹ ë¬¸ì„œ

                                    ## ğŸ“‹ ë©”íƒ€ë°ì´í„°
                                    - **ë¬¸ì„œ ì œëª©**: {filename}
                                    - **ì‘ì„±ì**: ë¯¸í™•ì¸
                                    - **ì‘ì„±ì¼**: {datetime.now().strftime('%Y-%m-%d')}
                                    - **ë²„ì „**: 1.0
                                    - **í”„ë¡œì íŠ¸/ì ìš© ë¶„ì•¼**: ë¯¸í™•ì¸
                                    - **ì£¼ìš” íƒœê·¸**: ë¶„ì„ ì¤‘

                                    ## ğŸ“– ë¬¸ì„œ ë³¸ë¬¸

                                    ### 1. ëª©ì  (Purpose)
                                    ì´ ë¬¸ì„œëŠ” ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œì„ í†µí•´ ë¶„ì„ëœ ë‚´ìš©ì„ ì •ë¦¬í•œ ë¬¸ì„œì…ë‹ˆë‹¤.

                                    ### 2. ì›ë³¸ ë‚´ìš©
                                    {analysis_result.get('original_content', 'ë‚´ìš© ì—†ìŒ')}

                                    ### 3. ë°œê²¬ëœ ê°œì„ ì‚¬í•­
                                    """
            for i, improvement in enumerate(analysis_result.get('improvements', []), 1):
                fallback_content += f"{i}. {improvement}\n"

            fallback_content += """
                                ### 4. ì ìš© ë° ì¬ì‚¬ìš© ë°©ì•ˆ
                                - í–¥í›„ ìœ ì‚¬í•œ í”„ë¡œì íŠ¸ì—ì„œ ì°¸ê³  ìë£Œë¡œ í™œìš© ê°€ëŠ¥
                                - ê°œì„ ì‚¬í•­ì„ ë°˜ì˜í•˜ì—¬ ë¬¸ì„œ í’ˆì§ˆ í–¥ìƒ í•„ìš”

                                ---
                                *ë³¸ ë¬¸ì„œëŠ” AI ì§€ì‹ê´€ë¦¬ ì‹œìŠ¤í…œì— ì˜í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
                                """

            enhanced_document = {
                "enhanced_content": fallback_content,
                "quality_score": analysis_result.get('quality_score', 70),
                "generation_metadata": {
                    "generated_at": datetime.now(),
                    "filename": filename,
                    "method": "Fallback Template Generation",
                    "version": "1.0",
                    "error": str(e)
                }
            }

            return enhanced_document

    def save_to_vector_db(self, analysis_result: Dict) -> Dict:
        """VectorDBì— ì €ì¥"""
        try:
            vector_entry = {
                "content": analysis_result['enhanced_content'],
                "filename": st.session_state.current_file_info['name'],
                "metadata": analysis_result.get('metadata', {}),
                "quality_score": analysis_result['quality_score'],
                "timestamp": datetime.now(),
                "original_length": analysis_result['original_length'],
                "analysis_type": st.session_state.get('analysis_settings', {}).get('depth', 'ê¸°ë³¸')
            }

            if 'vector_db' not in st.session_state:
                st.session_state.vector_db = []

            st.session_state.vector_db.append(vector_entry)

            return {
                "success": True,
                "message": "VectorDB ì €ì¥ ì™„ë£Œ",
                "count": len(st.session_state.vector_db)
            }

        except Exception as e:
            return {
                "success": False,
                "message": f"ì €ì¥ ì‹¤íŒ¨: {str(e)}"
            }

    def save_to_board(self, analysis_result: Dict) -> Dict:
        """ê²Œì‹œíŒì— ì €ì¥"""
        try:
            board_post = {
                "title": f"[AI ë³´ì™„] {st.session_state.current_file_info['name']}",
                "content": analysis_result['enhanced_content'],
                "author": "AI Knowledge System",
                "timestamp": datetime.now(),
                "views": 0,
                "quality_score": analysis_result['quality_score'],
                "file_info": st.session_state.current_file_info,
                "issues_found": analysis_result['issues_found'],
                "improvements": analysis_result['improvements'],
                "analysis_type": st.session_state.get('analysis_settings', {}).get('depth', 'ê¸°ë³¸')
            }

            if 'board_posts' not in st.session_state:
                st.session_state.board_posts = []

            st.session_state.board_posts.append(board_post)

            return {
                "success": True,
                "message": "ê²Œì‹œíŒ ë“±ë¡ ì™„ë£Œ",
                "count": len(st.session_state.board_posts)
            }

        except Exception as e:
            return {
                "success": False,
                "message": f"ë“±ë¡ ì‹¤íŒ¨: {str(e)}"
            }

    def bulk_vector_to_board(self) -> Dict:
        """VectorDB ë¬¸ì„œë“¤ì„ ê²Œì‹œíŒìœ¼ë¡œ ì¼ê´„ ì´ë™"""
        vector_db = st.session_state.get('vector_db', [])

        if not vector_db:
            return {
                "success": False,
                "message": "VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤",
                "count": 0
            }

        if 'board_posts' not in st.session_state:
            st.session_state.board_posts = []

        moved_count = 0
        for doc in vector_db:
            # ì´ë¯¸ ê²Œì‹œíŒì— ìˆëŠ”ì§€ í™•ì¸
            existing = any(
                post.get('title', '').endswith(doc.get('filename', ''))
                for post in st.session_state.board_posts
            )

            if not existing:
                board_post = {
                    "title": f"[VectorDB ì´ë™] {doc.get('filename', 'Unknown')}",
                    "content": doc.get('content', ''),
                    "author": "Knowledge Management System",
                    "timestamp": datetime.now(),
                    "views": 0,
                    "quality_score": doc.get('quality_score', 0),
                    "file_info": {"name": doc.get('filename', 'Unknown')},
                    "issues_found": [],
                    "improvements": [],
                    "source": "vector_db"
                }

                st.session_state.board_posts.append(board_post)
                moved_count += 1

        return {
            "success": True,
            "message": f"{moved_count}ê°œ ë¬¸ì„œ ì´ë™ ì™„ë£Œ",
            "count": moved_count
        }

    def get_knowledge_stats(self) -> Dict:
        """ì§€ì‹ ê´€ë¦¬ í†µê³„"""
        vector_db = st.session_state.get('vector_db', [])
        board_posts = st.session_state.get('board_posts', [])

        # í‰ê·  í’ˆì§ˆ ì ìˆ˜
        all_scores = []
        for doc in vector_db:
            if 'quality_score' in doc:
                all_scores.append(doc['quality_score'])
        for post in board_posts:
            if 'quality_score' in post:
                all_scores.append(post['quality_score'])

        avg_quality = sum(all_scores) / len(all_scores) if all_scores else 0

        # ì˜¤ëŠ˜ ë“±ë¡ëœ ë¬¸ì„œ ìˆ˜
        today = datetime.now().date()
        today_count = 0

        for doc in vector_db:
            if doc.get('timestamp') and doc['timestamp'].date() == today:
                today_count += 1

        for post in board_posts:
            if post.get('timestamp') and post['timestamp'].date() == today:
                today_count += 1

        return {
            'vector_count': len(vector_db),
            'board_count': len(board_posts),
            'avg_quality': avg_quality,
            'today_count': today_count,
            'total_documents': len(vector_db) + len(board_posts)
        }

    def get_file_info(self, uploaded_file) -> Dict:
        """íŒŒì¼ ì •ë³´ ë°˜í™˜"""
        return self.file_processor.get_file_info(uploaded_file)

    def search_knowledge(self, query: str) -> Dict:
        """ì§€ì‹ ê²€ìƒ‰"""
        vector_db = st.session_state.get('vector_db', [])
        board_posts = st.session_state.get('board_posts', [])

        results = {
            'vector_results': [],
            'board_results': [],
            'total_count': 0
        }

        query_lower = query.lower()

        # VectorDB ê²€ìƒ‰
        for doc in vector_db:
            if query_lower in doc.get('content', '').lower() or query_lower in doc.get('filename', '').lower():
                results['vector_results'].append(doc)

        # ê²Œì‹œíŒ ê²€ìƒ‰
        for post in board_posts:
            if query_lower in post.get('content', '').lower() or query_lower in post.get('title', '').lower():
                results['board_results'].append(post)

        results['total_count'] = len(results['vector_results']) + len(results['board_results'])
        return results


class RAGService:
    """RAGë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ë¶„í•  ë° ë²¡í„° DB ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, chroma_persist_directory: str = "./data/chroma_db",
                 sqlite_db_path: str = "./data/board.db"):
        """
        RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”

        Args:
            chroma_persist_directory: ChromaDB ì €ì¥ ê²½ë¡œ
            sqlite_db_path: SQLite ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        """
        # Azure OpenAI Embeddings ì´ˆê¸°í™”
        self.embeddings = AzureOpenAIEmbeddings(
            azure_endpoint=endpoint,
            api_key=api_key,
            api_version=embedding_api_version,
            azure_deployment=embedding_deployment
        )

        # ChromaDB ì´ˆê¸°í™”
        self.chroma_persist_directory = chroma_persist_directory
        os.makedirs(chroma_persist_directory, exist_ok=True)

        # SQLite DB ì´ˆê¸°í™”
        self.sqlite_db_path = sqlite_db_path
        os.makedirs(os.path.dirname(sqlite_db_path), exist_ok=True)
        self._initialize_board_db()

    def _initialize_board_db(self):
        """ê²Œì‹œíŒ DB í…Œì´ë¸” ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.sqlite_db_path)
        cursor = conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS board_posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                enhanced_doc_url TEXT,
                original_doc_url TEXT,
                author TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                views INTEGER DEFAULT 0,
                quality_score INTEGER DEFAULT 0,
                metadata TEXT
            )
        """)

        conn.commit()
        conn.close()

    def split_text(self, text: str, split_type: str = "semantic",
                   chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            split_type: ë¶„í•  ë°©ì‹ ("character", "recursive", "semantic", "token")
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°

        Returns:
            ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if split_type == "character":
                splitter = CharacterTextSplitter(
                    separator="\n\n",
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
            elif split_type == "recursive":
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    separators=["\n\n", "\n", " ", ""]
                )
            elif split_type == "semantic":
                splitter = SemanticChunker(
                    embeddings=self.embeddings,
                    breakpoint_threshold_type="gradient"
                )
            elif split_type == "token":
                splitter = TokenTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„í•  ë°©ì‹: {split_type}")

            chunks = splitter.split_text(text)
            return chunks

        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # Fallback: recursive splitter ì‚¬ìš©
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            return splitter.split_text(text)

    def embed_and_store(self, text: str, metadata: Dict = None,
                       split_type: str = "semantic",
                       chunk_size: int = 1000,
                       chunk_overlap: int = 200,
                       collection_name: str = "knowledge_base") -> Dict:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ë²¡í„° DBì— ì„ë² ë”©í•˜ì—¬ ì €ì¥

        Args:
            text: ì €ì¥í•  í…ìŠ¤íŠ¸
            metadata: ë©”íƒ€ë°ì´í„°
            split_type: ë¶„í•  ë°©ì‹
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„

        Returns:
            ì €ì¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self.split_text(text, split_type, chunk_size, chunk_overlap)

            if not chunks:
                return {
                    "success": False,
                    "message": "í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨",
                    "chunk_count": 0
                }

            # ë©”íƒ€ë°ì´í„° ì„¤ì •
            if metadata is None:
                metadata = {}

            # ê° ì²­í¬ì— ê³ ìœ  ID ë¶€ì—¬
            doc_id = metadata.get('doc_id', str(uuid.uuid4()))
            metadata['doc_id'] = doc_id

            # ChromaDBì— ì €ì¥
            vectorstore = Chroma.from_texts(
                texts=chunks,
                embedding=self.embeddings,
                metadatas=[{**metadata, 'chunk_id': i} for i in range(len(chunks))],
                collection_name=collection_name,
                persist_directory=self.chroma_persist_directory
            )

            return {
                "success": True,
                "message": "VectorDB ì €ì¥ ì™„ë£Œ",
                "chunk_count": len(chunks),
                "doc_id": doc_id,
                "collection_name": collection_name
            }

        except Exception as e:
            print(f"VectorDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "success": False,
                "message": f"ì €ì¥ ì‹¤íŒ¨: {str(e)}",
                "chunk_count": 0
            }

    def search_similar(self, query: str, k: int = 5,
                      collection_name: str = "knowledge_base") -> List[Dict]:
        """
        ìœ ì‚¬ë„ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜
            collection_name: ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ ì´ë¦„

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            vectorstore = Chroma(
                collection_name=collection_name,
                embedding_function=self.embeddings,
                persist_directory=self.chroma_persist_directory
            )

            results = vectorstore.similarity_search_with_score(query, k=k)

            formatted_results = []
            for doc, score in results:
                formatted_results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "similarity_score": float(score)
                })

            return formatted_results

        except Exception as e:
            print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def save_to_board_db(self, title: str, content: str,
                        enhanced_doc_url: str = None,
                        original_doc_url: str = None,
                        author: str = "AI Knowledge System",
                        quality_score: int = 0,
                        metadata: Dict = None) -> Dict:
        """
        ê²Œì‹œíŒ DBì— ë¬¸ì„œ ì €ì¥

        Args:
            title: ë¬¸ì„œ ì œëª©
            content: ë¬¸ì„œ ë‚´ìš© (ë³´ì™„ëœ ë‚´ìš©)
            enhanced_doc_url: ë³´ì™„ëœ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬
            original_doc_url: ì›ë³¸ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬
            author: ì‘ì„±ì
            quality_score: í’ˆì§ˆ ì ìˆ˜
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥)

        Returns:
            ì €ì¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            conn = sqlite3.connect(self.sqlite_db_path)
            cursor = conn.cursor()

            post_id = str(uuid.uuid4())
            import json

            # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if metadata:
                metadata_copy = {}
                for key, value in metadata.items():
                    if isinstance(value, dict):
                        # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
                        metadata_copy[key] = {}
                        for k, v in value.items():
                            if isinstance(v, datetime):
                                metadata_copy[key][k] = v.isoformat()
                            else:
                                metadata_copy[key][k] = v
                    elif isinstance(value, datetime):
                        metadata_copy[key] = value.isoformat()
                    else:
                        metadata_copy[key] = value
                metadata_json = json.dumps(metadata_copy, ensure_ascii=False)
            else:
                metadata_json = None

            cursor.execute("""
                INSERT INTO board_posts
                (id, title, content, enhanced_doc_url, original_doc_url,
                 author, quality_score, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (post_id, title, content, enhanced_doc_url, original_doc_url,
                  author, quality_score, metadata_json))

            conn.commit()
            conn.close()

            return {
                "success": True,
                "message": "ê²Œì‹œíŒ ì €ì¥ ì™„ë£Œ",
                "post_id": post_id
            }

        except Exception as e:
            print(f"ê²Œì‹œíŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "success": False,
                "message": f"ì €ì¥ ì‹¤íŒ¨: {str(e)}"
            }

    def get_board_posts(self, limit: int = 100, offset: int = 0) -> List[Dict]:
        """
        ê²Œì‹œíŒ ê²Œì‹œê¸€ ì¡°íšŒ

        Args:
            limit: ì¡°íšŒí•  ê²Œì‹œê¸€ ìˆ˜
            offset: ì‹œì‘ ìœ„ì¹˜

        Returns:
            ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
        """
        try:
            conn = sqlite3.connect(self.sqlite_db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            cursor.execute("""
                SELECT * FROM board_posts
                ORDER BY created_at DESC
                LIMIT ? OFFSET ?
            """, (limit, offset))

            rows = cursor.fetchall()
            conn.close()

            import json
            posts = []
            for row in rows:
                post = dict(row)
                if post.get('metadata'):
                    post['metadata'] = json.loads(post['metadata'])
                posts.append(post)

            return posts

        except Exception as e:
            print(f"ê²Œì‹œê¸€ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def get_board_post_by_id(self, post_id: str) -> Optional[Dict]:
        """
        íŠ¹ì • ê²Œì‹œê¸€ ì¡°íšŒ ë° ì¡°íšŒìˆ˜ ì¦ê°€

        Args:
            post_id: ê²Œì‹œê¸€ ID

        Returns:
            ê²Œì‹œê¸€ ì •ë³´
        """
        try:
            conn = sqlite3.connect(self.sqlite_db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # ì¡°íšŒìˆ˜ ì¦ê°€
            cursor.execute("""
                UPDATE board_posts
                SET views = views + 1
                WHERE id = ?
            """, (post_id,))

            # ê²Œì‹œê¸€ ì¡°íšŒ
            cursor.execute("""
                SELECT * FROM board_posts WHERE id = ?
            """, (post_id,))

            row = cursor.fetchone()
            conn.commit()
            conn.close()

            if row:
                import json
                post = dict(row)
                if post.get('metadata'):
                    post['metadata'] = json.loads(post['metadata'])
                return post

            return None

        except Exception as e:
            print(f"ê²Œì‹œê¸€ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def delete_board_post(self, post_id: str) -> Dict:
        """
        ê²Œì‹œê¸€ ì‚­ì œ

        Args:
            post_id: ê²Œì‹œê¸€ ID

        Returns:
            ì‚­ì œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            conn = sqlite3.connect(self.sqlite_db_path)
            cursor = conn.cursor()

            cursor.execute("DELETE FROM board_posts WHERE id = ?", (post_id,))

            conn.commit()
            deleted = cursor.rowcount > 0
            conn.close()

            if deleted:
                return {
                    "success": True,
                    "message": "ê²Œì‹œê¸€ ì‚­ì œ ì™„ë£Œ"
                }
            else:
                return {
                    "success": False,
                    "message": "ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                }

        except Exception as e:
            print(f"ê²Œì‹œê¸€ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "success": False,
                "message": f"ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
            }